
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Fundamentals &#8212; NVIDIA BioNeMo Framework</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="https://js.hcaptcha.com/1/api.js"></script>
    <script src="//assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.nvidia.com/bionemo-framework/0.4.0/bionemo-fw-for-model-training-fw.html" />
    <link rel="shortcut icon" href="_static/nvidia-logo-vert-rgb-blk-for-screen.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Data Module" href="data-module-fw.html" />
    <link rel="prev" title="BioNeMo Framework Tutorials" href="tutorial-list.html" /> 
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
      
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/nvidia-logo-horiz-rgb-blk-for-screen.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">NVIDIA BioNeMo Framework</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  GETTING STARTED
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   What is BioNeMo?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pre-reqs.html">
   Hardware and Software Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="access-startup.html">
   Access and Startup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="initialization-guide.html">
   Initialization Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tutorial-list.html">
   BioNeMo Framework Tutorials
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  BioNeMo CORE
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-module-fw.html">
   Data Module
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dwnstr-task-validation.html">
   Validation with a Downstream Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inference-triton-fw.html">
   Inference with Nvidia Triton Server
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="deep-dive-esm1-pytriton-inference.html">
   Example of PyTriton Inference
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  BEST PRACTICES
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="hyperparameters-fw.html">
   Hyperparameter Usage and Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="parallelism-fw.html">
   Training Parallelism
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MODELS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="models/diffdock.html">
   DiffDock
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/dnabert.html">
   DNABERT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/dsmbind.html">
   DSMBind
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/equidock.html">
   EquiDock
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/esm1-nv.html">
   ESM-1nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/esm2-nv.html">
   ESM-2nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/geneformer.html">
   Geneformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/megamolbart.html">
   MegaMolBART
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/molmim.html">
   MolMIM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/openfold.html">
   OpenFold
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/prott5nv.html">
   Prott5nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/model-benchmarks.html">
   Model Benchmarks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DATASETS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/uniprot.html">
   UniProt Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/CELLxGENE.html">
   CELLxGENE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/zinc15.html">
   ZINC-15 Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/pdb.html">
   The Protein Data Bank (PDB)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/GRCh38.p13.html">
   Human Reference Genome Version GRCh38.p13
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/openproteinset.html">
   OpenProteinSet
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/moleculenet-physchem.html">
   MoleculeNet Physical Chemistry Datasets - Lipophilicity, FreeSolv, ESOL
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  TUTORIALS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preprocessing-bcp-training-diffdock.html">
   DiffDock: Preparing Workspace and Data for Pre-training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/esm2nv-mutant-design.html">
   Zero-Shot Protein Design Using ESM-2nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/MMB_GenerativeAI_Inference_with_examples.html">
   BioNeMo - MegaMolBART Inferencing for Generative Chemistry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/MolMIM_GenerativeAI_local_inference_with_examples.html">
   MolMIM Inferencing for Generative Chemistry and Downstream Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/ZINC15-data-preprocessing.html">
   ZINC Training Dataset Setup for small molecule language models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/bionemo-finetuning-overview.html">
   Finetune pre-trained models in BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/cma_es_guided_molecular_optimization_molmim.html">
   MolMIM Property Guided Molecular Optimization Using CMA-ES
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/custom-dataset-class-fw.html">
   Adding the OAS Dataset: Modifying the Dataset Class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/custom-dataset-dataloader.html">
   Adding the OAS Dataset: Customizing Dataset Object and Dataloader Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/custom-dataset-preprocessing-fw.html">
   Adding the OAS Dataset: Downloading and Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/encoder-finetuning-notebook-fw.html">
   Encoder Fine-tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_diffdock.html">
   DiffDock Model Training using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_equidock.html">
   EquiDock Model Training using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_esm1nv.html">
   ESM1nv Model Training using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_esm2nv.html">
   ESM-2nv: Data Preprocessing and Model Training Using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_mmb.html">
   MegaMolBART Model Training using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_molmim.html">
   Train MolMIM from scratch on your own data using the BioNeMo Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/physchem-notebook-fw.html">
   Finetuning LLM in BioNeMo for a Downstream Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/protein-esm2nv-clustering.html">
   Generating Embeddings for Protein Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/retrosynthesis-notebook.html">
   Training LLM for a Custom Downstram Task: Retrosynthesis Prediction using MegaMolBART
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/esm2_FLIP_finetuning.html">
   Fine-Tune ESM-2nv on FLIP Data for Sequence-Level Classification, Regression, Token-Level Classification, and with LoRA Adapters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/esm2_oas_inferencing.html">
   Performing inference on OAS sequences with ESM-2nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/esm2_paratope_finetuning.html">
   Pretrain from Scratch, Continue Training from an Existing Checkpoint, and Fine-tune ESM-2nv on Custom Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/dnabert_inference.html">
   Generating and visualizing embeddings with DNABert
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/dnabert_pretrain_finetune.html">
   Pretrain, Fine-tune, and Perform Inference with DNABERT for Splice Site Prediction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  APPENDIX
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="faq-fw.html">
   Frequently Asked Questions (FAQ)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography-fw.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="releasenotes-fw.html">
   Release Notes
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview-core-features">
   Overview &amp; Core Features
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-new-models">
   Creating New Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#config-files">
   Config Files
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#command-line-configuration">
   Command Line Configuration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#checkpoints">
   Checkpoints
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-restore-connectors">
     Save-Restore Connectors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modifying-a-checkpoint-dictionary">
     Modifying a Checkpoint Dictionary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fine-tuning-pre-trained-checkpoints">
     Fine Tuning Pre-trained Checkpoints
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#datasets">
   Datasets
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#weights-biases-integration">
   Weights &amp; Biases Integration
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#api-setup">
     API Setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weights-and-biases-charts">
     Weights and Biases Charts
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Fundamentals</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview-core-features">
   Overview &amp; Core Features
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-new-models">
   Creating New Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#config-files">
   Config Files
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#command-line-configuration">
   Command Line Configuration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#checkpoints">
   Checkpoints
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-restore-connectors">
     Save-Restore Connectors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modifying-a-checkpoint-dictionary">
     Modifying a Checkpoint Dictionary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fine-tuning-pre-trained-checkpoints">
     Fine Tuning Pre-trained Checkpoints
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#datasets">
   Datasets
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#weights-biases-integration">
   Weights &amp; Biases Integration
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#api-setup">
     API Setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weights-and-biases-charts">
     Weights and Biases Charts
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                 <div class="tex2jax_ignore mathjax_ignore section" id="fundamentals">
<h1>Fundamentals<a class="headerlink" href="#fundamentals" title="Permalink to this headline">#</a></h1>
<p>In the sections below, key components of the BioNeMo framework and their use will be discussed</p>
<div class="section" id="overview-core-features">
<h2>Overview &amp; Core Features<a class="headerlink" href="#overview-core-features" title="Permalink to this headline">#</a></h2>
<p>The NVIDIA BioNeMo framework exists for training and deploying large biomolecular language models at supercomputing scale for the discovery and development of therapeutics. The large language model (LLM) framework currently has models for small molecules (SMILES) and protein sequences. Its modular design and high-level APIs make it easy to create, train, and deploy complex models for a variety of downstream tasks. The extensive collection of pre-trained models and scripts further facilitates rapid prototyping and development, and developers can customize the repository according to their needs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For detailed information on model pre-training or fine-tuning, users should consult the Tutorials section. This BioNeMo Core section is intended for advanced use cases.</p>
</div>
</div>
<div class="section" id="creating-new-models">
<h2>Creating New Models<a class="headerlink" href="#creating-new-models" title="Permalink to this headline">#</a></h2>
<p>BioNeMo models implement classes that inherit from <a class="reference external" href="https://github.com/NVIDIA/NeMo/tree/main/nemo/collections">NeMo Collections</a>. For example, ESM-1nv inherits functions from NeMo’s <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_bert_model.py">Megatron Bert Model</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ESM1nvModel</span><span class="p">(</span><span class="n">MegatronBertModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ESM1nv pre-training</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">DictConfig</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">)</span>

    <span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>Re-using code and architectures is the easiest way to modify existing BioNeMo pipelines. Creation of a new model in BioNeMo can be accomplished by leveraging NeMo Collections as a base model from which functions can be inherited. Similarly, existing BioNeMo implementations can be modified to create additional downstream tasks. This is the most straight-forward way to add additional building blocks to the repository. For example, in the snippet below found in <code class="docutils literal notranslate"><span class="pre">bionemo/model/protein/esm1nv/esm1nv_model.py</span></code> the tokenization method can be customized from the ESM-1nv implementation</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">_build_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default tokenizer is based on available nemo tokenizers.</span>
<span class="sd">        Override this method to use an external tokenizer.</span>
<span class="sd">        All tokenizers are expected to provide compatible interface.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_nmt_tokenizer</span><span class="p">(</span>
            <span class="n">library</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">library</span><span class="p">,</span>
            <span class="n">tokenizer_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">register_artifact</span><span class="p">(</span><span class="s2">&quot;tokenizer.model&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model</span><span class="p">),</span>
            <span class="n">vocab_file</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">register_artifact</span><span class="p">(</span><span class="s2">&quot;tokenizer.vocab_file&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_file</span><span class="p">),</span>
            <span class="n">legacy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>The same customization principles apply to all the other models available in BioNeMo framework, but certain nuances apply to each one of them according to their <code class="docutils literal notranslate"><span class="pre">super</span></code> class.  MegaMolBART, for example, does not have the same <code class="docutils literal notranslate"><span class="pre">&#64;property</span></code> decorated functions dealing with input and output names or input types as ESM1-nv:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ESM1-nv</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input_types</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;batch&#39;</span><span class="p">,</span>
                <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;time&#39;</span>
                <span class="p">},</span>
            <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;batch&#39;</span><span class="p">,</span>
                <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;time&#39;</span>
                <span class="p">}</span>
            <span class="p">}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_types</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;output&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;batch&#39;</span><span class="p">,</span>
                <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span>
                <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;size&#39;</span>
            <span class="p">}</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>On the other hand, the ESM-1nv implementation, which is based on a BERT (an encoder model), cannot sample and decode molecules as is done by MegaMolBART, which is based on BART (an encoder-decoder model).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># MegaMolBART</span>
<span class="k">def</span> <span class="nf">sample_molecules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens_enc</span><span class="p">,</span> <span class="n">enc_mask</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># Decode encoder hidden state to tokens</span>
        <span class="n">predicted_tokens_ids</span><span class="p">,</span> <span class="n">log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_enc</span><span class="p">,</span>
                                                      <span class="n">enc_mask</span><span class="p">,</span>
                                                      <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span>
                                                      <span class="n">enc_output</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">predicted_tokens_ids</span> <span class="o">=</span> <span class="n">predicted_tokens_ids</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Prune tokens by eos / padding and convert to SMILES</span>
        <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">predicted_tokens_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">predicted_tokens_ids</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_id</span> <span class="ow">in</span> <span class="n">predicted_tokens_</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">predicted_tokens_</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_id</span><span class="p">)</span>
                <span class="n">predicted_tokens_ids</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="o">=</span> <span class="n">predicted_tokens_</span><span class="p">[:</span><span class="n">idx</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># NB: this is slightly different from previous version in that pad tokens can be in the middle of sequence</span>
                <span class="n">predicted_tokens_ids</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nb">id</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">predicted_tokens_</span> <span class="k">if</span> <span class="nb">id</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_id</span><span class="p">]</span>

        <span class="n">predicted_tokens_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="p">(</span><span class="n">predicted_tokens_ids</span><span class="p">)</span>
        <span class="n">sampled_smiles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokens_to_text</span><span class="p">(</span><span class="n">predicted_tokens_text</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">sampled_smiles</span>
</pre></div>
</div>
<p>Here’s a summary of each model’s superclass:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Model</strong></p></th>
<th class="head"><p><strong>Superclass</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ESM-1nv</p></td>
<td><p>MegatronBertModel</p></td>
</tr>
<tr class="row-odd"><td><p>ProtT5nv</p></td>
<td><p>MegatronT5Model</p></td>
</tr>
<tr class="row-even"><td><p>MegaMolBART</p></td>
<td><p>MegatronLMEncoderDecoderModel</p></td>
</tr>
<tr class="row-odd"><td><p>MolMIM</p></td>
<td><p>MegatronLMEncoderDecoderModel</p></td>
</tr>
</tbody>
</table>
<p>Customizations can also vary from simple changes in the tokenizer methods to more involved changes in the model architecture, including alterations in how the forward step is computed, callbacks or data augmentation functions.</p>
</div>
<div class="section" id="config-files">
<h2>Config Files<a class="headerlink" href="#config-files" title="Permalink to this headline">#</a></h2>
<p>BioNeMo framework offers the option to easily set up and change model configurations for pre-training or fine-tuning workflows. Such configuration modifications can range from simple hyperparameters, such as number of hidden states to more advanced oodifications for data handling or connections to <a class="reference internal" href="#weights-and-biases-charts"><span class="std std-doc">Weights &amp; Biases Dashboards</span></a> for experiment tracking; for example, refer to  <code class="docutils literal notranslate"><span class="pre">bionemo/examples/protein/esm1nv/conf/base_config.yaml</span></code>. You can reference environment variables inside the configuration files using <a class="reference external" href="https://omegaconf.readthedocs.io/en/latest/custom_resolvers.html">OmegaConf resolvers</a>; for example the variable <code class="docutils literal notranslate"><span class="pre">$BIONEMO_HOME</span></code> maybe referenced via <code class="docutils literal notranslate"><span class="pre">${oc.env:BIONEMO_HOME}</span></code>.</p>
<p>Examples of typical configuration modifications for training can be fouind in the <em>.yaml</em> files within the framework</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">trainer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">devices</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w"> </span><span class="c1"># number of GPUs</span>
<span class="w">  </span><span class="nt">num_nodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># If you are working with a multi-node setting, for example, 2xDGX systems</span>
<span class="w">  </span><span class="nt">precision</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16-mixed</span><span class="w"> </span><span class="c1"># 16-mixed, bf16-mixed or 32</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">(...)</span>
<span class="w">  </span><span class="nt">accumulate_grad_batches</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</pre></div>
</div>
<p>BioNeMo also makes it easier to distribute weights for models that are too large for a single GPU onto multiple GPUs. A simple change in the <code class="docutils literal notranslate"><span class="pre">tensor_model_parallel_size</span></code> will make BioNeMo distribute a model’s layers across GPUs to better manage memory loads as weights are computed.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">micro_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">global_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">tensor_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"> </span><span class="c1"># Increase this number to distribute the model across GPUs</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">(...)</span>
<span class="w">  </span><span class="nt">seq_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">(...)</span>
<span class="w">  </span><span class="nt">num_attention_heads</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">12</span>
<span class="w">  </span><span class="nt">activation</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;gelu&#39;</span><span class="w"> </span><span class="c1"># Options [&#39;gelu&#39;, &#39;geglu&#39;, &#39;swiglu&#39;, &#39;reglu&#39;]</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">(...)</span>
</pre></div>
</div>
<p>It is also straightforward to change the configuration file to download and process datasets:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1"># Path to data must be specified by the user.</span>
<span class="w">    </span><span class="nt">data_url</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz&#39;</span>
<span class="w">    </span><span class="nt">dataset_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/data/uniref2022_05</span><span class="w"> </span><span class="c1"># parent directory for data, contains train / val / test folders</span>
<span class="w">    </span><span class="nt">dataset</span><span class="p">:</span><span class="w"> </span><span class="c1"># inclusive range of data files to load or can load a single file, for example, x000.csv</span>
<span class="w">      </span><span class="nt">train</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">x000</span>
<span class="w">      </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">x000</span>
<span class="w">      </span><span class="nt">val</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">x000</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">(...)</span>
<span class="w">    </span><span class="nt">seq_length_dec</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span><span class="w"> </span><span class="c1"># Target sequence length</span>
<span class="w">    </span><span class="nt">skip_warmup</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">num_workers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span><span class="w"> </span><span class="c1"># number of workers to be used for dataset preprocessing. 0 -- all available workers</span>
<span class="w">    </span><span class="nt">dataloader_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">single</span>
</pre></div>
</div>
<p>Some configuration parameters are inter-dependent. For example, <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code>, if provided, must be computed by a formula <code class="docutils literal notranslate"><span class="pre">micro_batch_size</span></code> * <code class="docutils literal notranslate"><span class="pre">devices</span></code> * <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> / (<code class="docutils literal notranslate"><span class="pre">tensor_model_parallel_size</span></code> * <code class="docutils literal notranslate"><span class="pre">pipeline_model_parallel_size</span></code>). For simplicity, <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> can be left to <code class="docutils literal notranslate"><span class="pre">null</span></code>, and the appropriate value will be inferred automatically.</p>
<p>There are also configuration parameters that are relevant to the model’s tasks and multiple config files can be set. For example, under <code class="docutils literal notranslate"><span class="pre">examples/molecule/megamolbart/</span></code> there are several config files, each of which establishing a specific behavior for pre-training tasks.</p>
<p>These configuration parameters should be reviewed prior to the first run to minimize chances of errors such as <em>CUDA Out of Memory</em> or low levels of GPU utilization in multi-node settings. For more information about configuration parameters check the files provided in the <code class="docutils literal notranslate"><span class="pre">conf</span></code> folder. To get more details about typical issues, refer to the <a class="reference internal" href="faq-fw.html"><span class="doc std std-doc">Frequently Asked Questions</span></a> or to the NVIDIA Developer Forums.</p>
<p>One additional feature of note is that ranges of data files can be selected, if the data are sharded into multiple files. For example, to select a contiguous range of 100 data files, from <code class="docutils literal notranslate"><span class="pre">x000.csv</span></code> to <code class="docutils literal notranslate"><span class="pre">x099.csv</span></code>, use the range indicator <code class="docutils literal notranslate"><span class="pre">x[000..099]</span></code>. For only ten files, use <code class="docutils literal notranslate"><span class="pre">x[000..009]</span></code>. Ensure these are set as appropriate for the train, validation, and test splits as below in the YAML config file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">data</span><span class="p">:</span>
<span class="w">    </span><span class="nt">dataset</span><span class="p">:</span>
<span class="w">      </span><span class="nt">train</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">x[000..099]</span>
<span class="w">      </span><span class="nt">test</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">x[000..099]</span>
<span class="w">      </span><span class="nt">val</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">x[000..099]</span>
</pre></div>
</div>
</div>
<div class="section" id="command-line-configuration">
<h2>Command Line Configuration<a class="headerlink" href="#command-line-configuration" title="Permalink to this headline">#</a></h2>
<p>BioNeMo framework users can count on resources improved management of containers and configuration settings of complex applications. There are two main components employed for that: NGC CLI and Hydra.</p>
<p>NVIDIA NGC CLI is a command-line interface tool for managing Docker containers in the NVIDIA NGC Registry. With NGC CLI, you can perform the same operations that are available from the NGC website, such as running jobs, viewing ACE and node information, and viewing Docker repositories within your orgs. For more information about how to set up CLI, check the <a class="reference external" href="https://ngc.nvidia.com/setup/installers/cli">NGC CLI SETUP</a> page and <a class="reference external" href="https://docs.ngc.nvidia.com/cli/index.html">NGC CLI Docs</a>.</p>
<p><a class="reference external" href="https://hydra.cc">Hydra</a> is a framework for simplifying the configuration of complex applications and environments. In the BioNeMo framework, the most common application of Hydra is to handle YAML-based configuration files and their parameters. <a class="reference external" href="https://yaml.org/spec/1.2.2/">YAML</a> is a human-readable data serialization standard that can be used in conjunction with all programming languages and is often used to write configuration files.</p>
<p>For example, to define training task using ESM-1nv, one could start the main method with a <code class="docutils literal notranslate"><span class="pre">&#64;hydra</span> <span class="pre">runner</span></code> decorator, defined by the creators of the NeMo Framework <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/main/nemo/core/config/hydra_runner.py">more details on GitHub</a>. This decorator is used for passing the config paths to main function and optionally registers a schema used for validation/providing default values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@hydra_runner</span><span class="p">(</span><span class="n">config_path</span><span class="o">=</span><span class="s2">&quot;&lt;configuration file path&gt;&quot;</span><span class="p">,</span> <span class="n">config_name</span><span class="o">=</span><span class="s2">&quot;&lt;name of the configuration&gt;&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">setup_trainer</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">do_training</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">ESM1nvModel</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">&#64;hydra_runner</span></code> Decorator can also help in multi-run configs. This can be useful for hyperparameter search and other tasks involving experimentation. To check detailed instructions about using <code class="docutils literal notranslate"><span class="pre">&#64;hydra_runner</span></code> check the <a class="reference external" href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/exp_manager.html">NeMo Experiment Manager documentation</a>.</p>
<p>Typically, the config files can be found under the <code class="docutils literal notranslate"><span class="pre">conf</span></code> directory in the same folder as the training script. For example, the config file for pre-training a <a class="reference external" href="https://arxiv.org/abs/2007.06225">ProtT5</a> model is located at <code class="docutils literal notranslate"><span class="pre">examples/protein/prott5nv/conf/pretrain_small.yaml</span></code>.</p>
</div>
<div class="section" id="checkpoints">
<h2>Checkpoints<a class="headerlink" href="#checkpoints" title="Permalink to this headline">#</a></h2>
<p>Pre-trained checkpoints are also provided based on the models described in the <a class="reference internal" href="index.html"><span class="doc std std-doc">Introduction</span></a>. These checkpoints will have token-size limitations.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Model</strong></p></th>
<th class="head"><p><strong>Checkpoint’s Max. Length</strong></p></th>
<th class="head"><p><strong>Common Tasks</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ESM-1nv &amp; ProtT5nv</p></td>
<td><p>512 Tokens (Amino Acids)</p></td>
<td><p>Thermostability, Secondary Structure Prediction, Subcellular Localization (check <a class="reference internal" href="models/model-benchmarks.html"><span class="doc std std-doc">FLIP Benchmark</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p>MegaMolBART</p></td>
<td><p>512 Tokens (SMILES)</p></td>
<td><p>Representation Learning, Structure Prediction, Molecule Generation</p></td>
</tr>
<tr class="row-even"><td><p>MolMIM</p></td>
<td><p>128 Tokens (SMILES)</p></td>
<td><p>Representation Learning, Structure Prediction, Molecule Generation, Guided molecular generation (see ./notebooks/cma_es_guided_molecular_optimization_molmim.ipynb)</p></td>
</tr>
</tbody>
</table>
<p>To enable support for longer sequences, customize configuration parameters to allow sequence lengths, assuming the user has the computational resources to support expansion. Taking ProtT5nv as example, several different configuration options are available in the <em>.yaml</em> file. Under <code class="docutils literal notranslate"><span class="pre">examples/protein/prott5nv/conf/</span></code> the <code class="docutils literal notranslate"><span class="pre">base_config.yaml</span></code> file is the basis for production-level work. More information about how to use checkpoints can be found in the <a class="reference internal" href="#save-restore-connectors"><span class="std std-doc">Save-Restore Connectors</span></a> section below.</p>
<div class="section" id="save-restore-connectors">
<h3>Save-Restore Connectors<a class="headerlink" href="#save-restore-connectors" title="Permalink to this headline">#</a></h3>
<p>The Save-Restore Connector (<code class="docutils literal notranslate"><span class="pre">BioNeMoSaveRestoreConnector</span></code>) is a component of BioNeMo framework. This class inherits functions from <code class="docutils literal notranslate"><span class="pre">NLPSaveRestoreConnector</span></code> from the NeMo Framework. These connectors are designed to handle the loading and modification of model checkpoints, for example, so that they can support changes in vocabulary size or a different type of positional embeddings. A Save-Restore connector object is supplied when initializing model from an existing checkpoint via the model <code class="docutils literal notranslate"><span class="pre">restore_from</span></code> method.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">restore_from_path</span></code> config parameter can be added to <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> files as one of the cutomizations mentioned before. For example, under <code class="docutils literal notranslate"><span class="pre">examples/conf/</span></code> the <code class="docutils literal notranslate"><span class="pre">base_infer_config.yaml</span></code> file has the following parameters:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">downstream_task</span><span class="p">:</span>
<span class="w">    </span><span class="nt">restore_from_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">???</span><span class="w"> </span><span class="c1"># path of pretrained model to be used in inference</span>
</pre></div>
</div>
<p>Which is invoked in several occasions, including in the <code class="docutils literal notranslate"><span class="pre">pretrain.py</span></code> file for the ProtT5nv model</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>   <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">do_training</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;************** Starting Training ***********&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">restore_from_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Restoring model from .nemo file &quot;</span> <span class="o">+</span> <span class="n">cfg</span><span class="o">.</span><span class="n">restore_from_path</span><span class="p">)</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">ProtT5nvModel</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
                <span class="n">cfg</span><span class="o">.</span><span class="n">restore_from_path</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">,</span>
                <span class="n">save_restore_connector</span><span class="o">=</span><span class="n">BioNeMoSaveRestoreConnector</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
                <span class="n">strict</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="modifying-a-checkpoint-dictionary">
<h3>Modifying a Checkpoint Dictionary<a class="headerlink" href="#modifying-a-checkpoint-dictionary" title="Permalink to this headline">#</a></h3>
<p>The save-restore connector is useful in situations when modifying the dictionary for an existing checkpoint, either to accommodate fewer or more tokens than used for the previous training. This allows flexibility when fine-tuning models with proprietary datasets.</p>
</div>
<div class="section" id="fine-tuning-pre-trained-checkpoints">
<h3>Fine Tuning Pre-trained Checkpoints<a class="headerlink" href="#fine-tuning-pre-trained-checkpoints" title="Permalink to this headline">#</a></h3>
<p>Given the ease to operate with <a class="reference internal" href="data-module-fw.html"><span class="doc std std-doc">Data Module</span></a> and <a class="reference internal" href="#save-restore-connectors"><span class="std std-doc">Save-Restore Connectors</span></a>, The BioNeMo Framework also makes it simple to organize and use checkpoints. A demo for implementing the EncoderFineTuning class from scratch, adding data, modifying the configuration parameters, and creating a fine tuning script can be found <a class="reference internal" href="notebooks/encoder-finetuning-notebook-fw.html"><span class="doc std std-doc">here</span></a>.</p>
</div>
</div>
<div class="section" id="datasets">
<h2>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">#</a></h2>
<p>BioNeMo framework pipelines are configured for specific dataset formats.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Model</strong></p></th>
<th class="head"><p><strong>Data Type</strong></p></th>
<th class="head"><p><strong>Common Public Datasets</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ESM-1nv &amp; ProtT5nv</p></td>
<td><p>Proteins  (FASTA, FASTQ, PDB)</p></td>
<td><p>UniProt, GenBank, SRA, PDB,</p></td>
</tr>
<tr class="row-odd"><td><p>MegaMolBART</p></td>
<td><p>Molecules (CSV or SMI)</p></td>
<td><p>PubChem, ZINC, ChEMBL, DUD-E</p></td>
</tr>
</tbody>
</table>
<p>However, the pipelines can be modified for different datasets. The following three tutorials describe how to modify the ESM-1nv pipeline for compatibility with the <a class="reference external" href="https://opig.stats.ox.ac.uk/webapps/oas/">Observed Antibody Space (OAS) database</a>:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="notebooks/custom-dataset-preprocessing-fw.html"><span class="doc std std-doc">Adding the OAS Dataset: Downloading and Preprocessing</span></a></p></li>
<li><p><a class="reference internal" href="notebooks/custom-dataset-class-fw.html"><span class="doc std std-doc">Adding the OAS Dataset: Modifying the Dataset Class</span></a></p></li>
<li><p>[Creating a Custom Dataloader](TUTORIAL LINK)</p></li>
</ol>
</div>
<div class="section" id="weights-biases-integration">
<h2>Weights &amp; Biases Integration<a class="headerlink" href="#weights-biases-integration" title="Permalink to this headline">#</a></h2>
<div class="section" id="api-setup">
<h3>API Setup<a class="headerlink" href="#api-setup" title="Permalink to this headline">#</a></h3>
<p>Enabling integration with Weights &amp; Biases is highly recommended. To leverage this feature, ensure the <code class="docutils literal notranslate"><span class="pre">.env</span></code> file contains a WANDB API key entered for the variable <code class="docutils literal notranslate"><span class="pre">WANDB_API_KEY</span></code>. Check the Weights and Biases user guide for further instructions on obtaining an API key. Refer to the <a class="reference internal" href="initialization-guide.html"><span class="doc std std-doc">Initialization Guide</span></a> section for details on setup.</p>
</div>
<div class="section" id="weights-and-biases-charts">
<h3>Weights and Biases Charts<a class="headerlink" href="#weights-and-biases-charts" title="Permalink to this headline">#</a></h3>
<p>In the image below are examples of metrics and dashboards used to monitor model training progress.</p>
<p><img alt="WANDB" src="_images/wandb-dashboard.png" /></p>
<p>These training and system related metrics are automatically logged in the online dashboard when an API key is provided and upload to Weights and Biases is enabled.</p>
<p>Training related metrics include:</p>
<ul class="simple">
<li><p><strong>Reduced Train Loss</strong>: is the value of the training loss function aggregated from all parallel processes. If the training loss doesn’t decrease or explodes, this is a possible sign that the learning rate needs to be reduced.</p></li>
<li><p><strong>Loss Scale</strong>: the scaling factor of the loss.</p></li>
<li><p><strong>Gradient Norm</strong>: the value of the gradient norm. Increasing or undefined (NaN) values of the gradient norm usually indicate instabilities in training. In such cases, the learning rate may need to be reduced.</p></li>
<li><p><strong>Learning Rate</strong>: the value of the learning rate.</p></li>
<li><p><strong>Epoch</strong>: the value of the epoch. Note: Megatron datasets upsample the data, so the entire training up to <code class="docutils literal notranslate"><span class="pre">max_steps</span></code> is considered a single epoch.</p></li>
<li><p><strong>Consumed Samples</strong>: the number of training samples that have been consumed during training.</p></li>
<li><p><strong>Validation Step Timing</strong>: monitors the time required for each validation step during training. this is useful for diagnosing performance issues and bottlenecks in the validation process, and can help optimize the speed and efficiency of model training.</p></li>
<li><p><strong>Train Backward Timing</strong>: a measure of the time required for the backpropagation step. Measuring the time this takes can help identify bottlenecks in the training process and assist in performance optimization.</p></li>
<li><p><strong>Validation Loss</strong>: the loss function computed on the validation set. Validation loss is used during model training to avoid over-fitting (when a model learns the training data too well and performs poorly on unseen data). If validation loss starts to increase while training loss decreases, this is usually a sign of over-fitting.</p></li>
</ul>
<p>Refer to also optional metrics for <a class="reference internal" href="dwnstr-task-validation.html"><span class="doc std std-doc">Validation With a Downstream Task</span></a></p>
</div>
</div>
</div>

<div class="section">
   
</div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="tutorial-list.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">BioNeMo Framework Tutorials</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="data-module-fw.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Data Module</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By NVIDIA<br/>
  
      &copy; Copyright 2023-2024 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved..<br/>
    Last updated on Nov 20, 2024.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>