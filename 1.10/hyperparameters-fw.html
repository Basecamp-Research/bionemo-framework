
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hyperparameter Usage and Tuning &#8212; NVIDIA BioNeMo Framework</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="https://js.hcaptcha.com/1/api.js"></script>
    <script src="//assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.nvidia.com/bionemo-framework/0.4.0/hyperparameters-fw.html" />
    <link rel="shortcut icon" href="_static/nvidia-logo-vert-rgb-blk-for-screen.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Training Parallelism" href="parallelism-fw.html" />
    <link rel="prev" title="Example of PyTriton Inference" href="deep-dive-esm1-pytriton-inference.html" /> 
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
      
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/nvidia-logo-horiz-rgb-blk-for-screen.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">NVIDIA BioNeMo Framework</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  GETTING STARTED
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   What is BioNeMo?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pre-reqs.html">
   Hardware and Software Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="access-startup.html">
   Access and Startup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="initialization-guide.html">
   Initialization Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tutorial-list.html">
   BioNeMo Framework Tutorials
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  BioNeMo CORE
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bionemo-fw-for-model-training-fw.html">
   Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-module-fw.html">
   Data Module
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dwnstr-task-validation.html">
   Validation with a Downstream Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inference-triton-fw.html">
   Inference with Nvidia Triton Server
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="deep-dive-esm1-pytriton-inference.html">
   Example of PyTriton Inference
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  BEST PRACTICES
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Hyperparameter Usage and Tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="parallelism-fw.html">
   Training Parallelism
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MODELS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="models/diffdock.html">
   DiffDock
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/dnabert.html">
   DNABERT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/dsmbind.html">
   Model Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/equidock.html">
   EquiDock
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/esm1-nv.html">
   ESM-1nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/esm2-nv.html">
   ESM-2nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/geneformer.html">
   Geneformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/megamolbart.html">
   MegaMolBART
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/molmim.html">
   MolMIM
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/openfold.html">
   OpenFold
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/prott5nv.html">
   Prott5nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models/model-benchmarks.html">
   Model Benchmarks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DATASETS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/uniprot.html">
   UniProt Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/CELLxGENE.html">
   CELLxGENE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/zinc15.html">
   ZINC-15 Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/pdb.html">
   The Protein Data Bank (PDB)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/GRCh38.p13.html">
   Human Reference Genome Version GRCh38.p13
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/openproteinset.html">
   OpenProteinSet
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets/moleculenet-physchem.html">
   MoleculeNet Physical Chemistry Datasets - Lipophilicity, FreeSolv, ESOL
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  TUTORIALS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preprocessing-bcp-training-diffdock.html">
   DiffDock: Preparing Workspace and Data for Pre-training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/esm2nv-mutant-design.html">
   Zero-Shot Protein Design Using ESM-2nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/MMB_GenerativeAI_Inference_with_examples.html">
   BioNeMo - MegaMolBART Inferencing for Generative Chemistry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/MolMIM_GenerativeAI_local_inference_with_examples.html">
   MolMIM Inferencing for Generative Chemistry and Downstream Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/ZINC15-data-preprocessing.html">
   ZINC Training Dataset Setup for small molecule language models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/bionemo-finetuning-overview.html">
   Finetune pre-trained models in BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/cma_es_guided_molecular_optimization_molmim.html">
   MolMIM Property Guided Molecular Optimization Using CMA-ES
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/custom-dataset-class-fw.html">
   Adding the OAS Dataset: Modifying the Dataset Class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/custom-dataset-dataloader.html">
   Adding the OAS Dataset: Customizing Dataset Object and Dataloader Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/custom-dataset-preprocessing-fw.html">
   Adding the OAS Dataset: Downloading and Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/encoder-finetuning-notebook-fw.html">
   Encoder Fine-tuning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_diffdock.html">
   DiffDock Model Training using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_equidock.html">
   EquiDock Model Training using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_esm1nv.html">
   ESM1nv Model Training using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_esm2nv.html">
   ESM-2nv: Data Preprocessing and Model Training Using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_mmb.html">
   MegaMolBART Model Training using BioNeMo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/model_training_molmim.html">
   Train MolMIM from scratch on your own data using the BioNeMo Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/physchem-notebook-fw.html">
   Finetuning LLM in BioNeMo for a Downstream Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/protein-esm2nv-clustering.html">
   Generating Embeddings for Protein Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/retrosynthesis-notebook.html">
   Training LLM for a Custom Downstram Task: Retrosynthesis Prediction using MegaMolBART
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/esm2_FLIP_finetuning.html">
   Fine-Tune ESM-2nv on FLIP Data for Sequence-Level Classification, Regression, Token-Level Classification, and with LoRA Adapters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/esm2_oas_inferencing.html">
   Performing inference on OAS sequences with ESM-2nv
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/esm2_paratope_finetuning.html">
   Pretrain from Scratch, Continue Training from an Existing Checkpoint, and Fine-tune ESM-2nv on Custom Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/dnabert_inference.html">
   Generating and visualizing embeddings with DNABert
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks/dnabert_pretrain_finetune.html">
   Pretrain, Fine-tune, and Perform Inference with DNABERT for Splice Site Prediction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  APPENDIX
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="faq-fw.html">
   Frequently Asked Questions (FAQ)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography-fw.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="releasenotes-fw.html">
   Release Notes
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-information">
   General Information
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameter-tuning-tips">
   Hyperparameter Tuning Tips
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recommended-hyperparameter-search-method">
   Recommended Hyperparameter Search Method
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#precision">
     Precision
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-clipping">
     Gradient Clipping
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimizer-and-weight-decay">
     Optimizer and Weight Decay
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-rate">
     Learning Rate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-size">
     Batch Size
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-parallelism">
     Model Parallelism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-training-tips">
   General Training Tips
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-norm">
     Gradient Norm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-cleaning">
     Data Cleaning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-architecture">
     Model Architecture
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization">
     Optimization
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Hyperparameter Usage and Tuning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-information">
   General Information
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameter-tuning-tips">
   Hyperparameter Tuning Tips
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recommended-hyperparameter-search-method">
   Recommended Hyperparameter Search Method
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#precision">
     Precision
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-clipping">
     Gradient Clipping
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimizer-and-weight-decay">
     Optimizer and Weight Decay
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-rate">
     Learning Rate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-size">
     Batch Size
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-parallelism">
     Model Parallelism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-training-tips">
   General Training Tips
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-norm">
     Gradient Norm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-cleaning">
     Data Cleaning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-architecture">
     Model Architecture
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization">
     Optimization
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                 <div class="tex2jax_ignore mathjax_ignore section" id="hyperparameter-usage-and-tuning">
<h1>Hyperparameter Usage and Tuning<a class="headerlink" href="#hyperparameter-usage-and-tuning" title="Permalink to this headline">#</a></h1>
<p>This section discusses recommended practices for choosing and tuning hyperparameters for BioNeMo models.</p>
<div class="section" id="general-information">
<h2>General Information<a class="headerlink" href="#general-information" title="Permalink to this headline">#</a></h2>
<p>Configuration files and command-line arguments can be used to define hyperparameters. Sets of configuration parameters are based on YAML files and constructed using <a class="reference external" href="https://hydra.cc/docs/intro/">Hydra</a>. Refer to the <a class="reference internal" href="bionemo-fw-for-model-training-fw.html#command-line-configuration"><span class="std std-doc">Command Line Configuration section</span></a> for more information.</p>
</div>
<div class="section" id="hyperparameter-tuning-tips">
<h2>Hyperparameter Tuning Tips<a class="headerlink" href="#hyperparameter-tuning-tips" title="Permalink to this headline">#</a></h2>
<p>There is a lot of information on how to tune hyperparameters (refer to comprehensive <a class="reference external" href="https://github.com/google-research/tuning_playbook">guide</a>). Here we provide a few tips that are specific to BioNeMo-based models.</p>
<ol class="arabic simple">
<li><p>Start small initially: dataset size, model parameters, number of epochs, and number of GPUs to tune hyperparameters.</p></li>
<li><p>Scale up experiment size gradually with best performing hyperparameters, for example, model size increases from 10M, 100M, 1B, 5B, 15B.</p></li>
<li><p>Use <a class="reference external" href="https://wandb.ai/">Weights &amp; Biases</a> to track experiment results. Group experiments by project per set of hyperparameters, and use meaningful names for experiments which include the parameters that have been varied. Stop experiments early if they are performing poorly.</p></li>
</ol>
</div>
<div class="section" id="recommended-hyperparameter-search-method">
<h2>Recommended Hyperparameter Search Method<a class="headerlink" href="#recommended-hyperparameter-search-method" title="Permalink to this headline">#</a></h2>
<p>Below, hyperparameters and recommendations for their adjustment are provided. The proposed values are generally applicable to large language models built upon NeMo Megatron, such as BioNeMo models.</p>
<div class="section" id="precision">
<h3>Precision<a class="headerlink" href="#precision" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Configure with: <code class="docutils literal notranslate"><span class="pre">trainer.precision=bf16-mixed</span></code> if available, otherwise use<code class="docutils literal notranslate"><span class="pre">trainer.precision=16-mixed</span></code>.</p></li>
<li><p>Switch to <code class="docutils literal notranslate"><span class="pre">trainer.precision=32</span></code> if training is unstable with bf16 or 16-bit.</p></li>
</ul>
</div>
<div class="section" id="gradient-clipping">
<h3>Gradient Clipping<a class="headerlink" href="#gradient-clipping" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Configure with: <code class="docutils literal notranslate"><span class="pre">trainer.gradient_clip_val=1.0</span></code></p></li>
<li><p>Recommended alternative values: 0.5, 0.1</p></li>
<li><p>Reduce value if training is unstable.</p></li>
</ul>
</div>
<div class="section" id="optimizer-and-weight-decay">
<h3>Optimizer and Weight Decay<a class="headerlink" href="#optimizer-and-weight-decay" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Configure with: <code class="docutils literal notranslate"><span class="pre">model.optim.name=fused_adam</span></code>, <code class="docutils literal notranslate"><span class="pre">model.optim.weight_decay=0.01</span></code></p></li>
<li><p>Recommended alternative values: <code class="docutils literal notranslate"><span class="pre">model.optim.weight_decay=0.0</span></code></p></li>
<li><p>Increase weight decay value to mitigate over-fitting and stabilize training. Values that become too large may degrade performance.</p></li>
</ul>
</div>
<div class="section" id="learning-rate">
<h3>Learning Rate<a class="headerlink" href="#learning-rate" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Configure with: <code class="docutils literal notranslate"><span class="pre">model.optim.lr=1e-4</span></code></p></li>
<li><p>Recommended alternative values: 2e-4, 5e-5</p></li>
<li><p>Instability in training or validation loss may indicate that the learning rate is too high. Slow convergence and poor performance of converged model may indicate that LR is too low.</p></li>
</ul>
</div>
<div class="section" id="batch-size">
<h3>Batch Size<a class="headerlink" href="#batch-size" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Configure with: <code class="docutils literal notranslate"><span class="pre">model.micro_batch_size=N</span></code> (per GPU batch size)</p></li>
<li><p>Recommended value: use <code class="docutils literal notranslate"><span class="pre">N</span></code> resulting in 85-90% GPU memory utilization</p></li>
<li><p>Keep <code class="docutils literal notranslate"><span class="pre">model.global_batch_size=null</span></code> to compute global batch size at run-time.</p></li>
<li><p>Further increase the effective global batch size by using gradient accumulation (for example, <code class="docutils literal notranslate"><span class="pre">trainer.accumulate_grad_batches=2</span></code>).</p></li>
</ul>
</div>
<div class="section" id="model-parallelism">
<h3>Model Parallelism<a class="headerlink" href="#model-parallelism" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>For large models (that is &gt; 1B parameters) use model tensor parallelism <code class="docutils literal notranslate"><span class="pre">model.tensor_model_parallel_size=N</span></code></p></li>
<li><p>For larger models (that is &gt; 5B parameters) add also model pipeline parallelism <code class="docutils literal notranslate"><span class="pre">model.pipeline_model_parallel_size=N</span></code></p></li>
<li><p>The various parallelism options are independent and can be combined as needed.</p></li>
</ul>
</div>
<div class="section" id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Configure with: <code class="docutils literal notranslate"><span class="pre">model.hidden_dropout=0.1</span></code>, <code class="docutils literal notranslate"><span class="pre">model.attention_dropout=0.1</span></code></p></li>
<li><p>Increase value to mitigate over-fitting. Values too large may degrade performance.</p></li>
</ul>
</div>
</div>
<div class="section" id="general-training-tips">
<h2>General Training Tips<a class="headerlink" href="#general-training-tips" title="Permalink to this headline">#</a></h2>
<div class="section" id="gradient-norm">
<h3>Gradient Norm<a class="headerlink" href="#gradient-norm" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>To mitigate spikes in gradient norm, reduce the gradient clipping value</p></li>
<li><p>Lower the learning rate, although this can also reduce performance and make training slow</p></li>
<li><p>Increase the number of warmup steps</p></li>
<li><p>Replace layer normalization with configuration <code class="docutils literal notranslate"><span class="pre">model.normalization=normformer</span></code></p></li>
<li><p>Increase global batch size (for example, using gradient accumulation)</p></li>
<li><p>Skip updates with large norm (for example, top 0.005% batches, leads to smoother loss)</p></li>
<li><p>For debugging: try <code class="docutils literal notranslate"><span class="pre">trainer.precision=32</span></code> to differentiate problems in numerical calculation vs. data batch</p></li>
</ul>
</div>
<div class="section" id="data-cleaning">
<h3>Data Cleaning<a class="headerlink" href="#data-cleaning" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Ensure data has been deduplicated to reduce memorization</p></li>
<li><p>Filter out irrelevant / bad quality data (for example invalid SMILES strings)</p></li>
</ul>
</div>
<div class="section" id="model-architecture">
<h3>Model Architecture<a class="headerlink" href="#model-architecture" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Pre-norm gives better performance but is less stable than post-ln. Normformer will be the most stable. Configure with <code class="docutils literal notranslate"><span class="pre">model.transformer_block_type</span></code> with options [‘pre_ln’, ‘post_ln’, ‘normformer’].</p></li>
<li><p>Model activaction of SwiGLU provides better performance at ~2% slowdown in training speed. Configure with <code class="docutils literal notranslate"><span class="pre">model.activation=swiglu</span></code>.</p></li>
<li><p>Remove dropouts, configure with <code class="docutils literal notranslate"><span class="pre">model.hidden_dropout=0.0</span></code>, <code class="docutils literal notranslate"><span class="pre">model.attention_dropout=0.0</span></code></p></li>
<li><p>Remove bias term from linear layers (increase stability and speed, almost no performance cost). Configure with <code class="docutils literal notranslate"><span class="pre">model.bias=false</span></code>.</p></li>
</ul>
</div>
<div class="section" id="optimization">
<h3>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Use 1-2k warmup steps with configuration <code class="docutils literal notranslate"><span class="pre">model.optim.sched.warmup_ratio=0.01</span></code> and <code class="docutils literal notranslate"><span class="pre">trainer.max_steps=100000</span></code>, or alternatively define warmup steps directly with configuration <code class="docutils literal notranslate"><span class="pre">model.optim.sched.warmup_steps=1000</span></code>.</p></li>
<li><p>Batch size ramp-up can be done via consecutive training with increased batch size, where previous model is used to initialize the weights of the next model. This can be done with configuration <code class="docutils literal notranslate"><span class="pre">restore_from_path=&lt;PATH</span> <span class="pre">TO</span> <span class="pre">.nemo</span> <span class="pre">FILE&gt;</span></code>.</p></li>
</ul>
</div>
</div>
</div>

<div class="section">
   
</div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="deep-dive-esm1-pytriton-inference.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Example of PyTriton Inference</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="parallelism-fw.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Training Parallelism</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By NVIDIA<br/>
  
      &copy; Copyright 2023-2024 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved..<br/>
    Last updated on Oct 17, 2024.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>