{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting biopython\n",
      "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (1.26.4)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, biopython, openpyxl\n",
      "Successfully installed biopython-1.85 et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from Bio import SeqIO\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from bionemo.noodles.nvfaidx import NvFaidx\n",
    "from bionemo.testing.data.fasta import ALU_SEQUENCE, create_fasta_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load brca_1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data if not present\n",
    "if not os.path.exists('brca1'):\n",
    "    os.makedirs('brca1')\n",
    "\n",
    "if not os.path.exists(os.path.join('brca1', '41586_2018_461_MOESM3_ESM.xlsx')):\n",
    "    !wget https://github.com/ArcInstitute/evo2/raw/refs/heads/main/notebooks/brca1/41586_2018_461_MOESM3_ESM.xlsx -O brca1/41586_2018_461_MOESM3_ESM.xlsx\n",
    "\n",
    "if not os.path.exists(os.path.join('brca1', 'GRCh37.p13_chr17.fna.gz')):\n",
    "    !wget https://github.com/ArcInstitute/evo2/raw/refs/heads/main/notebooks/brca1/GRCh37.p13_chr17.fna.gz -O brca1/GRCh37.p13_chr17.fna.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>score</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>41276135</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>-0.372611</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>41276135</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.045313</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>41276135</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>-0.108254</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>41276134</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>-0.277963</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>41276134</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.388414</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>41276134</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>-0.280973</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>41276133</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>-0.973683</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>41276133</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>-0.373489</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>41276133</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>41276132</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>-0.207552</td>\n",
       "      <td>FUNC/INT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chrom       pos ref alt     score     class\n",
       "0     17  41276135   T   G -0.372611  FUNC/INT\n",
       "1     17  41276135   T   C -0.045313  FUNC/INT\n",
       "2     17  41276135   T   A -0.108254  FUNC/INT\n",
       "3     17  41276134   T   G -0.277963  FUNC/INT\n",
       "4     17  41276134   T   C -0.388414  FUNC/INT\n",
       "5     17  41276134   T   A -0.280973  FUNC/INT\n",
       "6     17  41276133   C   T -0.973683  FUNC/INT\n",
       "7     17  41276133   C   G -0.373489  FUNC/INT\n",
       "8     17  41276133   C   A  0.006314  FUNC/INT\n",
       "9     17  41276132   A   T -0.207552  FUNC/INT"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brca1_df = pd.read_excel(\n",
    "    os.path.join('brca1', '41586_2018_461_MOESM3_ESM.xlsx'),\n",
    "    header=2,\n",
    ")\n",
    "brca1_df = brca1_df[[\n",
    "    'chromosome', 'position (hg19)', 'reference', 'alt', 'function.score.mean', 'func.class',\n",
    "]]\n",
    "\n",
    "# Rename columns\n",
    "brca1_df.rename(columns={\n",
    "    'chromosome': 'chrom',\n",
    "    'position (hg19)': 'pos',\n",
    "    'reference': 'ref',\n",
    "    'alt': 'alt',\n",
    "    'function.score.mean': 'score',\n",
    "    'func.class': 'class',\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert to two-class system\n",
    "brca1_df['class'] = brca1_df['class'].replace(['FUNC', 'INT'], 'FUNC/INT')\n",
    "\n",
    "brca1_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 8192\n",
    "\n",
    "# Read the reference genome sequence of chromosome 17\n",
    "with gzip.open(os.path.join('brca1', 'GRCh37.p13_chr17.fna.gz'), \"rt\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        seq_chr17 = str(record.seq)\n",
    "        break\n",
    "\n",
    "def parse_sequences(pos, ref, alt):\n",
    "    \"\"\"\n",
    "    Parse reference and variant sequences from the reference genome sequence.\n",
    "    \"\"\"\n",
    "    p = pos - 1 # Convert to 0-indexed position\n",
    "    full_seq = seq_chr17\n",
    "\n",
    "    ref_seq_start = max(0, p - WINDOW_SIZE//2)\n",
    "    ref_seq_end = min(len(full_seq), p + WINDOW_SIZE//2)\n",
    "    ref_seq = seq_chr17[ref_seq_start:ref_seq_end]\n",
    "    snv_pos_in_ref = min(WINDOW_SIZE//2, p)\n",
    "    var_seq = ref_seq[:snv_pos_in_ref] + alt + ref_seq[snv_pos_in_ref+1:]\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(var_seq) == len(ref_seq)\n",
    "    assert ref_seq[snv_pos_in_ref] == ref\n",
    "    assert var_seq[snv_pos_in_ref] == alt\n",
    "\n",
    "    return ref_seq, var_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring likelihoods of 1326 reference sequences with Evo 2...\n",
      "Scoring likelihoods of 3893 variant sequences with Evo 2...\n"
     ]
    }
   ],
   "source": [
    "# Build mappings of unique reference sequences\n",
    "ref_seqs = []\n",
    "ref_seq_to_index = {}\n",
    "\n",
    "# Parse sequences and store indexes\n",
    "ref_seq_indexes = []\n",
    "var_seqs = []\n",
    "\n",
    "for _, row in brca1_df.iterrows():\n",
    "    ref_seq, var_seq = parse_sequences(row['pos'], row['ref'], row['alt'])\n",
    "\n",
    "    # Get or create index for reference sequence\n",
    "    if ref_seq not in ref_seq_to_index:\n",
    "        ref_seq_to_index[ref_seq] = len(ref_seqs)\n",
    "        ref_seqs.append(ref_seq)\n",
    "    \n",
    "    ref_seq_indexes.append(ref_seq_to_index[ref_seq])\n",
    "    var_seqs.append(var_seq)\n",
    "\n",
    "ref_seq_indexes = np.array(ref_seq_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evo2\n",
    "\n",
    "We'll load evo2 weights from hugging face.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo W 2025-02-28 03:36:55 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Using byte-level tokenization\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: False\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1090: `trainer.init_module` cannot fully support proper instantiation of your model with the `MegatronStrategy` strategy. Please instantiate your model inside the`LightningModule.configure_model` hook instead\n",
      "\n",
      "[NeMo I 2025-02-28 03:36:58 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 03:36:59 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/local/bin/evo2_convert_to_nemo2\", line 8, in <module>\n",
      "[rank0]:     sys.exit(main())\n",
      "[rank0]:              ^^^^^^\n",
      "[rank0]:   File \"/workspace/bionemo2/sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_to_nemo.py\", line 61, in main\n",
      "[rank0]:     importer.apply(args.output_dir)\n",
      "[rank0]:   File \"/workspace/bionemo2/3rdparty/NeMo/nemo/collections/llm/gpt/model/hyena.py\", line 393, in apply\n",
      "[rank0]:     self.nemo_save(output_path, trainer)\n",
      "[rank0]:   File \"/workspace/bionemo2/3rdparty/NeMo/nemo/lightning/io/connector.py\", line 207, in nemo_save\n",
      "[rank0]:     trainer.save_checkpoint(output_path)\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1365, in save_checkpoint\n",
      "[rank0]:     self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
      "[rank0]:   File \"/workspace/bionemo2/3rdparty/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 778, in save_checkpoint\n",
      "[rank0]:     self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
      "[rank0]:   File \"/workspace/bionemo2/3rdparty/NeMo/nemo/lightning/io/pl.py\", line 200, in save_checkpoint\n",
      "[rank0]:     return dist_checkpointing.save(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/bionemo2/3rdparty/Megatron-LM/megatron/core/dist_checkpointing/serialization.py\", line 354, in save\n",
      "[rank0]:     raise CheckpointingException(\n",
      "[rank0]: megatron.core.dist_checkpointing.core.CheckpointingException: Checkpoint destination directory (nemo2_evo2_1b_8k/weights) is not empty\n"
     ]
    }
   ],
   "source": [
    "!evo2_convert_to_nemo2 --model-path hf://arcinstitute/savanna_evo2_1b_base --model-size 1b --output-dir nemo2_evo2_1b_8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to store a `Path` reference to where these checkpoints were saved so we can use them in the `predict` command later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: match path here with above command via shared string template\n",
    "checkpoint_path = Path(\"nemo2_evo2_1b_8k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to directories\n",
    "\n",
    "Convert both the reference and variant fasta sequences to their own respective `.fasta` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FRAC = 1.0\n",
    "if SAMPLE_FRAC < 1.0:\n",
    "    brca1_df = brca1_df.sample(frac=SAMPLE_FRAC).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique reference sequences: 1326\n",
      "Total unique variant sequences: 3893\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"brca1_fasta_files\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save reference and variant sequences to FASTA\n",
    "ref_fasta_path = output_dir / \"brca1_reference_sequences.fasta\"\n",
    "var_fasta_path = output_dir / \"brca1_variant_sequences.fasta\"\n",
    "\n",
    "# Track unique sequences\n",
    "ref_sequences = set()\n",
    "var_sequences = set()\n",
    "ref_seq_to_name = {}\n",
    "# Store unique sequences with metadata for writing\n",
    "ref_entries = []\n",
    "var_entries = []\n",
    "ref_names = []\n",
    "var_names = []\n",
    "# Collect unique reference and variant sequences\n",
    "for idx, row in brca1_df.iterrows():\n",
    "    ref_seq, var_seq = parse_sequences(row['pos'], row['ref'], row['alt'])\n",
    "\n",
    "    # Add to sets to ensure uniqueness\n",
    "    if ref_seq not in ref_sequences:\n",
    "        ref_sequences.add(ref_seq)\n",
    "        ref_name = f\"BRCA1_ref_pos_{row['pos']}_{row['ref']}_class_{row['class']}\"\n",
    "\n",
    "        ref_entries.append(\n",
    "            f\">{ref_name}\\n{ref_seq}\\n\"\n",
    "        )\n",
    "        ref_names.append(ref_name)\n",
    "        ref_seq_to_name[ref_seq] = ref_name\n",
    "    else:\n",
    "        ref_name = ref_seq_to_name[ref_seq]\n",
    "        ref_names.append(ref_name)\n",
    "    if var_seq not in var_sequences:\n",
    "        var_sequences.add(var_seq)\n",
    "        var_name = f\"BRCA1_var_pos_{row['pos']}_{row['ref']}to{row['alt']}_class_{row['class']}\"\n",
    "\n",
    "        var_entries.append(\n",
    "            f\">{var_name}\\n{var_seq}\\n\"\n",
    "        )\n",
    "        var_names.append(var_name)\n",
    "    else:\n",
    "        assert False, \"Duplicate variant sequence\"\n",
    "\n",
    "# Write unique sequences to FASTA files\n",
    "with open(ref_fasta_path, \"w\") as f:\n",
    "    f.writelines(ref_entries)\n",
    "\n",
    "with open(var_fasta_path, \"w\") as f:\n",
    "    f.writelines(var_entries)\n",
    "\n",
    "# Print counts\n",
    "print(f\"Total unique reference sequences: {len(ref_sequences)}\")\n",
    "print(f\"Total unique variant sequences: {len(var_sequences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_df['ref_fasta_name'] = ref_names\n",
    "brca1_df['var_fasta_name'] = var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Selected Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>BRCA1_ref_pos_41276135_T_class_FUNC/INT\\nTTAAACACTTTTCAAACCAGGCAATATTTTAGGCCTACTGTATATTTGCATTTTGAGCTTCCAATACGGATAAGTGACTGGAAAAAGCAGCTAGGTTTAGGTTGAAAAACAACAACCCACCGGGGAACACATTTTAGCAAATTCTTCTGAAAGTCAAAAATGTTATAGTCATAGGTAAAAAGTTACAAAGAACTACCAATTGTCAGAAATAGCTGCCAATATTGACTTAGAAGACAGCAGAAGGAATTTTAGTTCAAGAAACCTAAAACAGGCTGAAAACCTTACCTACCCTATAGCTACCACAAATAACACTGTTTCCAGTCATGATCATTCCTGATCACATATTAAGACATAACTGCAAATTGTGCTATACTGTACTATATTAAAAGGAAGTGAAATATGATCCCTATCCTAGAACTTTCCATACAAATGAATGTAAAACACCATAAAAATTAATCTTAAGGCCGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGTGGGCGGATCACGAGGTCAGGAAGTGGAGACCATCCTGGCTAACACGGTGAAACCCCGTCTCTACTAAAAATACAAAAAATTAGCCGGGCGTGGTGGTGGACGCCTGTAGTCCCAGCTACTTGGGGGGCCGAGGCAGGAGAATGGCGTGAACCCGGGAGGCGGAGCTTGCAGTGAGCCGAGATGGCGCCACTGCACTCCGGCCTGGGTGAAAGAGCGAGACTCCGTCTCAAAAACAAAACAAACAAAAATTAATCTTAAGCCAGGCGCAGTGGCTCACGCCAGCACTTTGGAAGGCCGAGGCGGGTGGATCACGAGATCAGGACTTCAAGACCAGCCTGACCAACGTGATGAAACCCTATCTCTACTAAAAATACAAAATTAGCCGGCCACGGTGGCGTGCGCCTATAATCCCAGCTACTCAGGAGGCTGAGGCAGGAGAAGCGCTTGAACTTGAACCTGGCAGGCGGAGGTTGCAGTGAGCCAAGATGGCGCCACTGCACTCCAGCCTGGGCGACAGAGCCAGACTCCAACCCCCCACCCCGAAAAAAAAAGGTCCAGGCCGGGCGCAGTGGCTCAGGACTGTAATCCCAGCACTTTGGAAGGCTGAGGCGGGTGGATCACAAGGTCAGGAGATCGAGACCATCTTGGCTAACATGGTGAAACCCCGTCTCTACTAAAAATACAAAAAATTAGCCGGGCATAGTGGTGGGCGCCTGTAGTCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATGGCCTGAACCCGGGAGGCGGAGCTGGCAGTGAGCCAAGATCGTGCCACTGCACTCCAGCCTAGGCAGCAGAGCGAGACCGTGTCTCAAAAAAACAAAACAAAACAAAACAAAAAGTCTGGGAGCGGTGGCTCACGCCTGTAATCCCAGCACTTTCGGAGGCCAAGGCAGGAGGATCACCTGAGGTCAGGAGTTCGAGACCAACCTGACCAATATGGAGAAACCCTGTCTCTACTAAAAATACAAAATTAGCTGGTGTGATGGCACATGCCTGCAATCCCAGGTACTCCGGAGGCTGAGGCAGCAGAATTGCTTGAACCCGGGAGGTGGAGGTTGTAGTGAGCCGAGATTGTGCCACTGCACTCCAGCCTGGGCAACAAGAGCCAAAGTCTGTCTCAAAAAAAAAAAAAAAAAAAAAAAAAGAAATTAATCTTAACAGGAAACAGAAAAAAGCAATGAAAAGCTAGAAAACATAATAGTTGATTGAAAATAACAATTTAGCATTTTCATTCTTACATCTTTAATTTTTATGTATCTGAGTTTTTAATTGATGGTTTAATTTGCCAGAATGAGAAAGAACATCCTATTTTTATGACTCTCTCCCATGGAAATGAAACATAAATGTATCCAAATGCCACACTATTGAGGATTTTCCTGATCACTGATTGTCATGAGTAAGTTTTGTGCTTTTTCAAAAGCAGTTTTTTCCTACAATGTCATTTCCTGCTTCTCTGGCTCTGATTTTCAATAAATTGATAAATTGTGAATCCTGTTTTCCTCTTATTTTTGTTTAGCTATAATGTTGAAGGGCAAGGGAGAGGATGGTTATTTATAAATCTTGTATCGCTCTGAAAACACAACATACATTTTCCTTAATCTGATTAACTTGACTTCAAATATGAAAAACAACTTTCATAAAGCAGAAAAGAATTTACCCTTTTTTATTGTGGGTAAGAGGCAATGGTACAACTTTTCAACTTATTTTTTGAATGTTACTCACTACTAACCATCACCATATTTAAAAAAATTAAAGAACTAATTTAGTTTAGTTTATTATTTATTTGACAAATGTTTATTGAGTGGCAACTAGGTCCCAAGTACCGTTCTAACTACTGAACATACAGATGTATGTAAACAAAACAAAAATCCCATCCTGGAGTTTACATTCTGTGGGACTAGAGATAAAAAATGGATACATTACATAGAATGTCAGCTAGTAATCAGTGTTATGGAGAAGCAGCAGGAATAGAAGATAAAGTGTGTGCTGGGGGTGTGGTAATTTTAAATAGGGGTGTCTGGAAATGAAAAGGTGGTATTTCAATCAAGATTTTTAGACCATGGCTGGGTGCAATGGCTCAGGCCTGTAATCCCAGCACCTTGAGAGGCCAAGGGAGGGTAGATCACTTGAGGTCAGGAGTTTGAGACCAGCATGGCCAACATAGCAAAACCCTATCTCTACAACAGAAAAATACAAGAATGGCTGGACGCAGTGGCTTATGCCTGTAATCCTAGCACTTTGGGAGGCCCAGGCGGGTGGATCACAAGGTCAGGAGATCAAGACTATCCTGGCTAACACGGTGAAATCCCGCCTCTACTAAAAAAGAAAAAAAAATACAAAAAATTAGCCGGGCGTGGTAGTGGGTGCTTGTAGTCCCAGCTATTCAGGAGGCTCAGGCAGAAGAATGGCATGAACCCGGGAGGCAGAGTTTGCAGTGAGCTGAGATCGCGCCACTGCACTCCAGCCTGGGCAACAGAGCAAGACTCCATCTCAAAAAAAGAAAAAAAAATACAAAAATTAGCTGGGCATGGTGGTGCACACCTATCGTCCCTGCTACTCTGGAGGCTGAGGTGGGAGGATTGCTTGAGCCTGACGAGGTTGAGGCTGCAGTGAGCTGTGATAGCACCACTGCACTCCAGCCTCTCGACAGAGATCCTATATAAAAAAAAAACCTCTGCATTTCATTGTATGTAAATAAGTATGTAATTTCATTGTATGTACAGAGCCAGTTTCAAACAAAGGTTCTTCCAAATACCTATCCTCTCAACGACACCGATCATCCATGTTTTTTTTTTTTTTTTTTTTTTTTTGAGATGGAGTTTAGCTCTGTCGCTGGAGTTCAGTGGTGCCATATTGGCTCACAGCAACATCTGCCTCCTGGTTCAAGTGATTCTCCTGCCTCAGCCTCCTGAGTAGCTGGGATTACAGGCACATGCCACTACGCCCAGCTAATTTTTGTATTTTTAGTGGAGAGGGGGTTTCACCATGTTGGCCAGGATGGTCTCGATCTCCTGACCTCGTGATCCTACCACCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATAAGCCACCGCCCTCGGCCTCATCCATGATTTTATTTTGCCATTTCAAGTGATGGAGCTTGTTTTAGAGCTGGAAGAAAAGCCAAAATGCCAGTTAATCTAAACTAGATTCCTGCCCCAGTGCAGAACCAATCAAGACAGAGTCCCTGTCTTTCCCGGACCACAGGATTTGTGTTGAAAAGGAGAGGAGTGGGAGAGGCAGAGTGGATGGAGAACAAGGAATCATTTTCTATATTTTTAAAGTTCTTCAGTTAAGAAAATCAGCAATTACAATAGCCTAATCTTACTAGACATGTCTTTTCTTCCCTAGTATGTAAGGTCAATTCTGTTCATTTGCATAGGAGATAATCATAGGAATCCCAAATTAATACACTCTTGTGCTGACTTACCAGATGGGACACTCTAAGATTTTCTGCATAGCATTAATGACATTTTGTACTTCTTCAACGCGAAGAGCAGATAAATCCATTTCTTTCTGTTCCAATGAACTTTAACACATTAGAAAAACATATATATATATCTTTTTAAAAGGTTTATAAAATGACAACTTCATTTTATCATTTTAAAATAAAGTAAATTTAAGATTTGGAAGGTTTTAGAATAATACAAACCAAAGAACTAATGACAACGTCCTTTATTTTTAAAGATTCTAGAAGTTGCTTTTTGTAATTAGACAACATAAATTCTGAATTTTTTCACATATTGCTGCCAACCCCTTGGGTCTTTTCCTTTCTCCAAGAAAGAGAAAGCTACAGAGGAGTGACTGACCGGGTAGGTGGTGGTAGCCTTAGCTTTCTCCAATGTTTCTGGTTGTTTTCTTTTTCTTGCATAAAACCAAAATCAACAACGACCAAACCAACACCAATCAAGGCCTCCCCGCCCCTAACCTTTCCCAGTGACCTGCTCTCATCTCTGGATCCTCCTCAAGCACATCCCTGCCGGCAGCATCTGTTACTACTGACGCTCCTCTACTTCCCTCTTGCGCTTTCTCAATGGCGCAAATGGATCCAGTTCTTAAGTTCTCCCTCCCACAAAATCCTGTCTCCTCCCCTTCCCAGACATATTCCTGGCACCTCTTCTTCCACAAGGTCCCATCCTCTCATACATACCAGCCGGTGTTTTTTGTTTTGTTTTGTTTTGTTTTGTTTTGAGACAGTCTCGCTCTGTCGCCCAGGCTGGAGTGCAATGGCGCGATCTCGGCTCACTGCAACCTCCGCCTCCCGGGTTCTAGCGATTCTCCTGCCTCAGCCTCCTGAGTAGCTGGAGCGGCACCACGCCCGGCTAATTTTTGTATTTTTAGTAGAGACGGAGTTTCACCACGTTGGTCAGGCTGGTCTGGAACTCCTGACCTCATGACCAGCCGACGTTTTTAAAGACATAGTGTCCCCCTCAAGGCATATTCCAGTTCCTATCACGAGGATTCCCCCACGGACACTCAGTGCCCCCTTCCTGATCCTCAGCGCTTCCCTCGCGACCTACAAACTGCCCCCCTCCCCAGGGTTCACAACGCCTTACGCCTCTCAGGTTCCGCCCCTACCCCCCGTCAAAGAATACCCATCTGTCAGCTTCGGAAATCCACTCTCCCACGCCAGTACCCCAGAGCATCACTTGGGCCCCCTGTCCCTTTCCCGGGACTCTACTACCTTTACCCAGAGCAGAGGGTGAAGGCCTCCTGAGCGCAGGGGCCCAGTTATCTGAGAAACCCCACAGCCTGTCCCCCGTCCAGGAAGTCTCAGCGAGCTCACGCCGCGCAGTCGCAGTTTTAATTTATCTGTAATTCCCGCGCTTTTCCGTTGCCACGGAAACCAAGGGGCTACCGCTAAGCAGCAGCCTCTCAGAATACGAAATCAAGGTACAATCAGAGGATGGGAGGGACAGAAAGAGCCAAGCGTCTCTCGGGGCTCTGGATTGGCCACCCAGTCTGCCCCCGGATGACGTAAAAGGAAAGAGACGGAAGAGGAAGAATTCTACCTGAGTTTGCCATAAAGTGCCTGCCCTCTAGCCTCTACTCTTCCAGTTGCGGCTTATTGCATCACAGTAATTGCTGTACGAAGGTCAGAATCGCTACCTATTGTCCAAAGCAGTCGTAAGAAGAGGTCCCAATCCCCCACTCTTTCCGCCCTAATGGAGGTCTCCAGTTTCGGTAAATATAAGTAATAAGGATTGTTGGGGGGGTGGAGGGAAATAATTATTTCCAGCATGCGTTGCGGAATGAAAGGTCTTCGCCACAGTGTTCCTTAGAAACTGTAGTCTTATGGAGAGGAACATCCAATACCAGAGCGGGCACAATTCTCACGGAAATCCAGTGGATAGATTGGAGACCTGTGCGCGCTTGTACTTGTCAACAGTTATGGACTGGAGTGTTATGTTTTCGTATTTTGAAAGCAGAAACTAGGCCTTAAAAAGATACGTACAACTCTTTAGGGAGACTACAATTCCCATCCAGCCCCAGGAGTCTGGGGCAAGTAGTCTTGTAAGGTCAGTGGCCTGCGGGGACGCAGTGAGCGCCGAATTTGCCTGGGGCAGGGGAAATGCGCTCTGGCCCATGTCTGCGCACTCGTAGTTCCACCCCTCAGCCCCAGTGTTTGTTATTTTTCGGGTTCAGCTTGCTTTTGCCCCGTCTCCGTCGACGCAATCGCCACCAGTCAATGGGGTGGTCGTTTTGAGGGACAAGTGGTAAGAGCCAATCTTCTTGGCGAAAACGCGGAGAAACGGGACTAGTTACTGTCTTTGTCCGCCATGTTAGATTCACCCCACAGAGATAGCGGCAGAGCTGGCAGCGGACGGTCTTTGCATTGCCGCCTCCCCAGGGGGCGGGAAGCTGGTAAGGAAGCAGCCTGGGTTAGCTAGGGGTGGGGTCACGTCACACTAAGAGGGTTTGGAGAAGTTCAAGGGAGGAATCCTGCAAAGAAGAGGGGCGACTTTTTCCGTGTCTCCGGACAGCTAATCGTTTTAGTGACAGGATGAGAGAGCCCTTCGTGTTCTGAGGGACCGAGTGGGCGAAAAGCGCCGGAGAGTTGGAGAGTCTGTGGTTCAGAATGCGAGGTGACAACGTGCTAGCAGCCCTCGCTCGCTCTCGGCGCCTCCTCGGCCTTGGCGTCCATTCTGGCCGTGCTGGAGGAGCCCTTCAGCCCGCCACTGCGCTGTGGGGGCCCCTCTCTGGGCTGGCCGAAGCCAGAGCCGGCTCCCTCTGCTTGCGGGGAAGTGTGGAGGGAGAGGCGGGTGTGGGAACTGGGGCTGCGCGCAGCGCTCGCCAGCCAGCGCGAGTTCCAGGTGGGCGCGGGCTCAGCGGGCCCCGCACCCCCGGCCCCGGGCAGTCAGGGGCCTAGCACCCGGGCCAGCAGCTGCAGAGGGTGCGCCGGGTCCCCCAGCACTGCCGGCCCGCCTGCACCCCGCTTGAATTCTCACCGGGCCCCAGCCGCCCTGCACAGGGCAAGGCTCAGGACCTGCAGCCCGCCATGCCCGAGCCCCCTCCCAACCCCTGTGAGCTCCAGCGTGGCCTGAGCCTCCCCGACGGGCACCGCCCCCTGCTCCTCAGCGCCCGGTCCCATCGACTGCCCAAGGGCTGAGAGGAGTGCAGGCGCCCGGCACAGCCCTGCGCAGGATCCACTAGGTGAAGCCAGCTGGGCTCCTGAGTCAGATGGGGACTTGGAAAACTTTTATGTCTAGCCTGAGGATTTTATATGCACCAGTCAGCACTCTGTGTCTAGCTTGGGGTTTGGGGATGCACCAATCAGCACTCTGTATCTAGCTAATCTGGTGGGCACTTGGAGAACTTCTGTGTCTAGCTAAAGGATTGTAAATGCACCAATCAGTGCTCTGTGTCTAGCTCAAGGTTTGCAAATGCACCAATCAGCACTCTGTGTCTAGCTAAAGGTTTGTAAACGCACCAATCAGTGCTCTGTGTCTAGCAAATGTAGTGGGGACTTGGAGAATTTTTATGTCTAGCTAGAGGATTGTAAATGCACCAATCAGCACTCTGTGTATATCTAGCTCAGGGATTGTAAATGCACCAATCAGCACCCTGTCAAAACGGACCAATTAGCTCTCTGTAAAATGGACCAATCAACAGGATGTGGGTGGGGTCAGATAAGGGAATAAAAGCAGGCTGCCCCGCTGGGTGCCAGTGGCTCACACCTGTAATCCCAGCAATTTGGGAGGCCTAGAGGGGTGGATCACGAGGTCAAGAGATCGAGACCATCCTGGCTAACACAGTGAAACCCCGACTCTACTAAAAAGACAAAATATTAGCTGGGTGCGGTGGTGGGTGCCTGTAATCCCCTCTACTGGGGAGGTTGAGGCAGGAGAATGGCGTGAACCCGGGAGGCGGAGCTTGCAGTGAGCCCAGATTGCACCACTGCATTCCAGCCTGGGTGACAGAGGGAGACTCCATCTCAAAAAAAAAAAAAAAAAAAAAAAAAAATGCAGGCTGCCTGAGCCAGCAGCAGCAACCCGCTCTGGTCTCCTTCCACGCTGTGGAAGCTTTGTTCTTGTGCTCTTTGCAATAAATCTTGCTGCTGCTCACTCTTTGGGTCCGCATAGCATTTATCTGCTGGTAACACCGACCGCAGAGGTCTGCAGCTTCA\\n',\n",
       " '>BRCA1_ref_pos_41276134_T_class_FUNC/INT\\nTTTAAACACTTTTCAAACCAGGCAATATTTTAGGCCTACTGTATATTTGCATTTTGAGCTTCCAATACGGATAAGTGACTGGAAAAAGCAGCTAGGTTTAGGTTGAAAAACAACAACCCACCGGGGAACACATTTTAGCAAATTCTTCTGAAAGTCAAAAATGTTATAGTCATAGGTAAAAAGTTACAAAGAACTACCAATTGTCAGAAATAGCTGCCAATATTGACTTAGAAGACAGCAGAAGGAATTTTAGTTCAAGAAACCTAAAACAGGCTGAAAACCTTACCTACCCTATAGCTACCACAAATAACACTGTTTCCAGTCATGATCATTCCTGATCACATATTAAGACATAACTGCAAATTGTGCTATACTGTACTATATTAAAAGGAAGTGAAATATGATCCCTATCCTAGAACTTTCCATACAAATGAATGTAAAACACCATAAAAATTAATCTTAAGGCCGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGTGGGCGGATCACGAGGTCAGGAAGTGGAGACCATCCTGGCTAACACGGTGAAACCCCGTCTCTACTAAAAATACAAAAAATTAGCCGGGCGTGGTGGTGGACGCCTGTAGTCCCAGCTACTTGGGGGGCCGAGGCAGGAGAATGGCGTGAACCCGGGAGGCGGAGCTTGCAGTGAGCCGAGATGGCGCCACTGCACTCCGGCCTGGGTGAAAGAGCGAGACTCCGTCTCAAAAACAAAACAAACAAAAATTAATCTTAAGCCAGGCGCAGTGGCTCACGCCAGCACTTTGGAAGGCCGAGGCGGGTGGATCACGAGATCAGGACTTCAAGACCAGCCTGACCAACGTGATGAAACCCTATCTCTACTAAAAATACAAAATTAGCCGGCCACGGTGGCGTGCGCCTATAATCCCAGCTACTCAGGAGGCTGAGGCAGGAGAAGCGCTTGAACTTGAACCTGGCAGGCGGAGGTTGCAGTGAGCCAAGATGGCGCCACTGCACTCCAGCCTGGGCGACAGAGCCAGACTCCAACCCCCCACCCCGAAAAAAAAAGGTCCAGGCCGGGCGCAGTGGCTCAGGACTGTAATCCCAGCACTTTGGAAGGCTGAGGCGGGTGGATCACAAGGTCAGGAGATCGAGACCATCTTGGCTAACATGGTGAAACCCCGTCTCTACTAAAAATACAAAAAATTAGCCGGGCATAGTGGTGGGCGCCTGTAGTCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATGGCCTGAACCCGGGAGGCGGAGCTGGCAGTGAGCCAAGATCGTGCCACTGCACTCCAGCCTAGGCAGCAGAGCGAGACCGTGTCTCAAAAAAACAAAACAAAACAAAACAAAAAGTCTGGGAGCGGTGGCTCACGCCTGTAATCCCAGCACTTTCGGAGGCCAAGGCAGGAGGATCACCTGAGGTCAGGAGTTCGAGACCAACCTGACCAATATGGAGAAACCCTGTCTCTACTAAAAATACAAAATTAGCTGGTGTGATGGCACATGCCTGCAATCCCAGGTACTCCGGAGGCTGAGGCAGCAGAATTGCTTGAACCCGGGAGGTGGAGGTTGTAGTGAGCCGAGATTGTGCCACTGCACTCCAGCCTGGGCAACAAGAGCCAAAGTCTGTCTCAAAAAAAAAAAAAAAAAAAAAAAAAGAAATTAATCTTAACAGGAAACAGAAAAAAGCAATGAAAAGCTAGAAAACATAATAGTTGATTGAAAATAACAATTTAGCATTTTCATTCTTACATCTTTAATTTTTATGTATCTGAGTTTTTAATTGATGGTTTAATTTGCCAGAATGAGAAAGAACATCCTATTTTTATGACTCTCTCCCATGGAAATGAAACATAAATGTATCCAAATGCCACACTATTGAGGATTTTCCTGATCACTGATTGTCATGAGTAAGTTTTGTGCTTTTTCAAAAGCAGTTTTTTCCTACAATGTCATTTCCTGCTTCTCTGGCTCTGATTTTCAATAAATTGATAAATTGTGAATCCTGTTTTCCTCTTATTTTTGTTTAGCTATAATGTTGAAGGGCAAGGGAGAGGATGGTTATTTATAAATCTTGTATCGCTCTGAAAACACAACATACATTTTCCTTAATCTGATTAACTTGACTTCAAATATGAAAAACAACTTTCATAAAGCAGAAAAGAATTTACCCTTTTTTATTGTGGGTAAGAGGCAATGGTACAACTTTTCAACTTATTTTTTGAATGTTACTCACTACTAACCATCACCATATTTAAAAAAATTAAAGAACTAATTTAGTTTAGTTTATTATTTATTTGACAAATGTTTATTGAGTGGCAACTAGGTCCCAAGTACCGTTCTAACTACTGAACATACAGATGTATGTAAACAAAACAAAAATCCCATCCTGGAGTTTACATTCTGTGGGACTAGAGATAAAAAATGGATACATTACATAGAATGTCAGCTAGTAATCAGTGTTATGGAGAAGCAGCAGGAATAGAAGATAAAGTGTGTGCTGGGGGTGTGGTAATTTTAAATAGGGGTGTCTGGAAATGAAAAGGTGGTATTTCAATCAAGATTTTTAGACCATGGCTGGGTGCAATGGCTCAGGCCTGTAATCCCAGCACCTTGAGAGGCCAAGGGAGGGTAGATCACTTGAGGTCAGGAGTTTGAGACCAGCATGGCCAACATAGCAAAACCCTATCTCTACAACAGAAAAATACAAGAATGGCTGGACGCAGTGGCTTATGCCTGTAATCCTAGCACTTTGGGAGGCCCAGGCGGGTGGATCACAAGGTCAGGAGATCAAGACTATCCTGGCTAACACGGTGAAATCCCGCCTCTACTAAAAAAGAAAAAAAAATACAAAAAATTAGCCGGGCGTGGTAGTGGGTGCTTGTAGTCCCAGCTATTCAGGAGGCTCAGGCAGAAGAATGGCATGAACCCGGGAGGCAGAGTTTGCAGTGAGCTGAGATCGCGCCACTGCACTCCAGCCTGGGCAACAGAGCAAGACTCCATCTCAAAAAAAGAAAAAAAAATACAAAAATTAGCTGGGCATGGTGGTGCACACCTATCGTCCCTGCTACTCTGGAGGCTGAGGTGGGAGGATTGCTTGAGCCTGACGAGGTTGAGGCTGCAGTGAGCTGTGATAGCACCACTGCACTCCAGCCTCTCGACAGAGATCCTATATAAAAAAAAAACCTCTGCATTTCATTGTATGTAAATAAGTATGTAATTTCATTGTATGTACAGAGCCAGTTTCAAACAAAGGTTCTTCCAAATACCTATCCTCTCAACGACACCGATCATCCATGTTTTTTTTTTTTTTTTTTTTTTTTTGAGATGGAGTTTAGCTCTGTCGCTGGAGTTCAGTGGTGCCATATTGGCTCACAGCAACATCTGCCTCCTGGTTCAAGTGATTCTCCTGCCTCAGCCTCCTGAGTAGCTGGGATTACAGGCACATGCCACTACGCCCAGCTAATTTTTGTATTTTTAGTGGAGAGGGGGTTTCACCATGTTGGCCAGGATGGTCTCGATCTCCTGACCTCGTGATCCTACCACCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATAAGCCACCGCCCTCGGCCTCATCCATGATTTTATTTTGCCATTTCAAGTGATGGAGCTTGTTTTAGAGCTGGAAGAAAAGCCAAAATGCCAGTTAATCTAAACTAGATTCCTGCCCCAGTGCAGAACCAATCAAGACAGAGTCCCTGTCTTTCCCGGACCACAGGATTTGTGTTGAAAAGGAGAGGAGTGGGAGAGGCAGAGTGGATGGAGAACAAGGAATCATTTTCTATATTTTTAAAGTTCTTCAGTTAAGAAAATCAGCAATTACAATAGCCTAATCTTACTAGACATGTCTTTTCTTCCCTAGTATGTAAGGTCAATTCTGTTCATTTGCATAGGAGATAATCATAGGAATCCCAAATTAATACACTCTTGTGCTGACTTACCAGATGGGACACTCTAAGATTTTCTGCATAGCATTAATGACATTTTGTACTTCTTCAACGCGAAGAGCAGATAAATCCATTTCTTTCTGTTCCAATGAACTTTAACACATTAGAAAAACATATATATATATCTTTTTAAAAGGTTTATAAAATGACAACTTCATTTTATCATTTTAAAATAAAGTAAATTTAAGATTTGGAAGGTTTTAGAATAATACAAACCAAAGAACTAATGACAACGTCCTTTATTTTTAAAGATTCTAGAAGTTGCTTTTTGTAATTAGACAACATAAATTCTGAATTTTTTCACATATTGCTGCCAACCCCTTGGGTCTTTTCCTTTCTCCAAGAAAGAGAAAGCTACAGAGGAGTGACTGACCGGGTAGGTGGTGGTAGCCTTAGCTTTCTCCAATGTTTCTGGTTGTTTTCTTTTTCTTGCATAAAACCAAAATCAACAACGACCAAACCAACACCAATCAAGGCCTCCCCGCCCCTAACCTTTCCCAGTGACCTGCTCTCATCTCTGGATCCTCCTCAAGCACATCCCTGCCGGCAGCATCTGTTACTACTGACGCTCCTCTACTTCCCTCTTGCGCTTTCTCAATGGCGCAAATGGATCCAGTTCTTAAGTTCTCCCTCCCACAAAATCCTGTCTCCTCCCCTTCCCAGACATATTCCTGGCACCTCTTCTTCCACAAGGTCCCATCCTCTCATACATACCAGCCGGTGTTTTTTGTTTTGTTTTGTTTTGTTTTGTTTTGAGACAGTCTCGCTCTGTCGCCCAGGCTGGAGTGCAATGGCGCGATCTCGGCTCACTGCAACCTCCGCCTCCCGGGTTCTAGCGATTCTCCTGCCTCAGCCTCCTGAGTAGCTGGAGCGGCACCACGCCCGGCTAATTTTTGTATTTTTAGTAGAGACGGAGTTTCACCACGTTGGTCAGGCTGGTCTGGAACTCCTGACCTCATGACCAGCCGACGTTTTTAAAGACATAGTGTCCCCCTCAAGGCATATTCCAGTTCCTATCACGAGGATTCCCCCACGGACACTCAGTGCCCCCTTCCTGATCCTCAGCGCTTCCCTCGCGACCTACAAACTGCCCCCCTCCCCAGGGTTCACAACGCCTTACGCCTCTCAGGTTCCGCCCCTACCCCCCGTCAAAGAATACCCATCTGTCAGCTTCGGAAATCCACTCTCCCACGCCAGTACCCCAGAGCATCACTTGGGCCCCCTGTCCCTTTCCCGGGACTCTACTACCTTTACCCAGAGCAGAGGGTGAAGGCCTCCTGAGCGCAGGGGCCCAGTTATCTGAGAAACCCCACAGCCTGTCCCCCGTCCAGGAAGTCTCAGCGAGCTCACGCCGCGCAGTCGCAGTTTTAATTTATCTGTAATTCCCGCGCTTTTCCGTTGCCACGGAAACCAAGGGGCTACCGCTAAGCAGCAGCCTCTCAGAATACGAAATCAAGGTACAATCAGAGGATGGGAGGGACAGAAAGAGCCAAGCGTCTCTCGGGGCTCTGGATTGGCCACCCAGTCTGCCCCCGGATGACGTAAAAGGAAAGAGACGGAAGAGGAAGAATTCTACCTGAGTTTGCCATAAAGTGCCTGCCCTCTAGCCTCTACTCTTCCAGTTGCGGCTTATTGCATCACAGTAATTGCTGTACGAAGGTCAGAATCGCTACCTATTGTCCAAAGCAGTCGTAAGAAGAGGTCCCAATCCCCCACTCTTTCCGCCCTAATGGAGGTCTCCAGTTTCGGTAAATATAAGTAATAAGGATTGTTGGGGGGGTGGAGGGAAATAATTATTTCCAGCATGCGTTGCGGAATGAAAGGTCTTCGCCACAGTGTTCCTTAGAAACTGTAGTCTTATGGAGAGGAACATCCAATACCAGAGCGGGCACAATTCTCACGGAAATCCAGTGGATAGATTGGAGACCTGTGCGCGCTTGTACTTGTCAACAGTTATGGACTGGAGTGTTATGTTTTCGTATTTTGAAAGCAGAAACTAGGCCTTAAAAAGATACGTACAACTCTTTAGGGAGACTACAATTCCCATCCAGCCCCAGGAGTCTGGGGCAAGTAGTCTTGTAAGGTCAGTGGCCTGCGGGGACGCAGTGAGCGCCGAATTTGCCTGGGGCAGGGGAAATGCGCTCTGGCCCATGTCTGCGCACTCGTAGTTCCACCCCTCAGCCCCAGTGTTTGTTATTTTTCGGGTTCAGCTTGCTTTTGCCCCGTCTCCGTCGACGCAATCGCCACCAGTCAATGGGGTGGTCGTTTTGAGGGACAAGTGGTAAGAGCCAATCTTCTTGGCGAAAACGCGGAGAAACGGGACTAGTTACTGTCTTTGTCCGCCATGTTAGATTCACCCCACAGAGATAGCGGCAGAGCTGGCAGCGGACGGTCTTTGCATTGCCGCCTCCCCAGGGGGCGGGAAGCTGGTAAGGAAGCAGCCTGGGTTAGCTAGGGGTGGGGTCACGTCACACTAAGAGGGTTTGGAGAAGTTCAAGGGAGGAATCCTGCAAAGAAGAGGGGCGACTTTTTCCGTGTCTCCGGACAGCTAATCGTTTTAGTGACAGGATGAGAGAGCCCTTCGTGTTCTGAGGGACCGAGTGGGCGAAAAGCGCCGGAGAGTTGGAGAGTCTGTGGTTCAGAATGCGAGGTGACAACGTGCTAGCAGCCCTCGCTCGCTCTCGGCGCCTCCTCGGCCTTGGCGTCCATTCTGGCCGTGCTGGAGGAGCCCTTCAGCCCGCCACTGCGCTGTGGGGGCCCCTCTCTGGGCTGGCCGAAGCCAGAGCCGGCTCCCTCTGCTTGCGGGGAAGTGTGGAGGGAGAGGCGGGTGTGGGAACTGGGGCTGCGCGCAGCGCTCGCCAGCCAGCGCGAGTTCCAGGTGGGCGCGGGCTCAGCGGGCCCCGCACCCCCGGCCCCGGGCAGTCAGGGGCCTAGCACCCGGGCCAGCAGCTGCAGAGGGTGCGCCGGGTCCCCCAGCACTGCCGGCCCGCCTGCACCCCGCTTGAATTCTCACCGGGCCCCAGCCGCCCTGCACAGGGCAAGGCTCAGGACCTGCAGCCCGCCATGCCCGAGCCCCCTCCCAACCCCTGTGAGCTCCAGCGTGGCCTGAGCCTCCCCGACGGGCACCGCCCCCTGCTCCTCAGCGCCCGGTCCCATCGACTGCCCAAGGGCTGAGAGGAGTGCAGGCGCCCGGCACAGCCCTGCGCAGGATCCACTAGGTGAAGCCAGCTGGGCTCCTGAGTCAGATGGGGACTTGGAAAACTTTTATGTCTAGCCTGAGGATTTTATATGCACCAGTCAGCACTCTGTGTCTAGCTTGGGGTTTGGGGATGCACCAATCAGCACTCTGTATCTAGCTAATCTGGTGGGCACTTGGAGAACTTCTGTGTCTAGCTAAAGGATTGTAAATGCACCAATCAGTGCTCTGTGTCTAGCTCAAGGTTTGCAAATGCACCAATCAGCACTCTGTGTCTAGCTAAAGGTTTGTAAACGCACCAATCAGTGCTCTGTGTCTAGCAAATGTAGTGGGGACTTGGAGAATTTTTATGTCTAGCTAGAGGATTGTAAATGCACCAATCAGCACTCTGTGTATATCTAGCTCAGGGATTGTAAATGCACCAATCAGCACCCTGTCAAAACGGACCAATTAGCTCTCTGTAAAATGGACCAATCAACAGGATGTGGGTGGGGTCAGATAAGGGAATAAAAGCAGGCTGCCCCGCTGGGTGCCAGTGGCTCACACCTGTAATCCCAGCAATTTGGGAGGCCTAGAGGGGTGGATCACGAGGTCAAGAGATCGAGACCATCCTGGCTAACACAGTGAAACCCCGACTCTACTAAAAAGACAAAATATTAGCTGGGTGCGGTGGTGGGTGCCTGTAATCCCCTCTACTGGGGAGGTTGAGGCAGGAGAATGGCGTGAACCCGGGAGGCGGAGCTTGCAGTGAGCCCAGATTGCACCACTGCATTCCAGCCTGGGTGACAGAGGGAGACTCCATCTCAAAAAAAAAAAAAAAAAAAAAAAAAAATGCAGGCTGCCTGAGCCAGCAGCAGCAACCCGCTCTGGTCTCCTTCCACGCTGTGGAAGCTTTGTTCTTGTGCTCTTTGCAATAAATCTTGCTGCTGCTCACTCTTTGGGTCCGCATAGCATTTATCTGCTGGTAACACCGACCGCAGAGGTCTGCAGCTTC\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_entries[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "selected_dir = Path(\"brca1_selected_seqs\")\n",
    "selected_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save reference and variant sequences to FASTA\n",
    "selected_fasta_path = selected_dir / \"brca1_selected_sequences.fasta\"\n",
    "\n",
    "with open(selected_fasta_path, \"w\") as f:\n",
    "    f.writelines(ref_entries[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Score Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_selected_seq_command = (\n",
    "    f\"predict_evo2 --fasta {selected_fasta_path} --ckpt-dir {checkpoint_path} \"\n",
    "    f\"--output-dir {selected_dir} --model-size 1b --tensor-parallel-size 1 \"\n",
    "    \"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs --fp8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo W 2025-02-28 04:16:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-02-28 04:16:58 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Experiments will be logged at /tmp/tmp8z8pmr3_/default\n",
      "[NeMo W 2025-02-28 04:16:58 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /tmp/tmp8z8pmr3_\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Using byte-level tokenization\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2025-02-28 04:16:58 num_microbatches_calculator:228] setting number of microbatches to constant 1\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:16:58 random:220] CPU RNG state changed within GPU RNG context\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "[NeMo W 2025-02-28 04:16:58 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1108204800\n",
      "[NeMo I 2025-02-28 04:16:58 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, average_in_collective=False, fp8_param_gather=False)\n",
      "[NeMo I 2025-02-28 04:16:58 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "    Params for bucket 1 (1108204800 elements):\n",
      "    \tmodule.decoder.layers.17.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.12.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.1.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.23.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.21.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.13.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.10.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.8.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.6.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.19.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.16.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.14.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.11.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.6.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.4.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.23.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.20.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.20.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.15.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.12.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.5.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.4.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.23.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.22.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.16.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.8.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.7.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.22.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.21.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.16.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.14.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.12.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.9.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.4.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.18.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.10.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.0.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.24.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.22.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.19.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.16.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.14.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.21.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.12.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.8.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.6.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.4.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.21.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.18.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.16.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.13.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.9.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.6.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.2.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.23.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.20.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.16.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.15.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.10.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.18.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.23.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.21.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.15.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.6.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.embedding.word_embeddings.weight\n",
      "    \tmodule.decoder.layers.22.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.12.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.9.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.7.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.17.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.23.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.20.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.20.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.15.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.13.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.8.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.5.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.5.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.21.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.19.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.14.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.9.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.18.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.22.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.20.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.19.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.14.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.9.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.7.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.5.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.16.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.13.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.11.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.24.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.22.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.19.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.16.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.14.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.12.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.9.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.7.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.20.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.20.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.18.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.15.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.13.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.5.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.5.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.5.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.1.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.18.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.16.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.13.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.11.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.9.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.6.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.22.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.20.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.14.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.9.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.8.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.1.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.23.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.18.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.15.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.13.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.11.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.8.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.4.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.24.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.19.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.4.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.17.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.20.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.15.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.13.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.10.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.8.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.6.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.5.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.22.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.18.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.13.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.12.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.10.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.7.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.23.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.19.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.12.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.11.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.7.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.4.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.1.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.24.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.15.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.14.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.9.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.6.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.final_norm.weight\n",
      "    \tmodule.decoder.layers.19.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.16.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.12.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.9.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.7.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.4.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.23.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.21.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.20.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.17.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.17.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.11.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.8.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.6.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.22.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.11.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.9.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.6.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.0.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.23.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.19.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.15.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.13.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.7.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.5.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.24.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.23.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.21.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.16.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.13.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.11.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.8.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.6.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.conv_bias\n",
      "[NeMo I 2025-02-28 04:16:58 nemo_logging:393] Doing selective restore from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False)\n",
      "[WARNING  | py.warnings        ]: /workspace/bionemo2/3rdparty/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "  checkpoint.load_state_dict(\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "  device = getattr(value, \"device\", None)\n",
      "\n",
      "[NeMo I 2025-02-28 04:16:59 nemo_logging:393] Restoring model weights from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False)\n",
      "[NeMo I 2025-02-28 04:16:59 nemo_logging:393] Finished restoring from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False), cleaning up.\n"
     ]
    }
   ],
   "source": [
    "!{predict_selected_seq_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Forward Pass Logits\n",
    "\n",
    "Note, before running this, to get the `input_ids`, add the following to `predict.py`'s return statement in predict_step:\n",
    "\n",
    "```\n",
    "            return {\n",
    "                \"token_logits\": forward_out.cpu(),\n",
    "                \"pad_mask\": batch[\"loss_mask\"].cpu(),\n",
    "                \"seq_idx\": batch[\"seq_idx\"].cpu(),\n",
    "                \"input_ids\": batch[\"tokens\"].cpu()\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_logits_dir = selected_dir / \"logits\"\n",
    "selected_logits_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# just forward pass\n",
    "predict_selected_seq_logits_command = (\n",
    "    f\"predict_evo2 --fasta {selected_fasta_path} --ckpt-dir {checkpoint_path} \"\n",
    "    f\"--output-dir {selected_logits_dir} --model-size 1b --tensor-parallel-size 1 \"\n",
    "    \"--pipeline-model-parallel-size 1 --context-parallel-size 1 --fp8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "[NeMo W 2025-02-28 04:17:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-02-28 04:17:58 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Experiments will be logged at /tmp/tmp2_7kwhk7/default\n",
      "[NeMo W 2025-02-28 04:17:58 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /tmp/tmp2_7kwhk7\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Using byte-level tokenization\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-02-28 04:17:58 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2025-02-28 04:17:58 num_microbatches_calculator:228] setting number of microbatches to constant 1\n",
      "[NeMo I 2025-02-28 04:17:59 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "[NeMo W 2025-02-28 04:17:59 random:220] CPU RNG state changed within GPU RNG context\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "[NeMo W 2025-02-28 04:17:59 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "[NeMo I 2025-02-28 04:17:59 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1108204800\n",
      "[NeMo I 2025-02-28 04:17:59 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, average_in_collective=False, fp8_param_gather=False)\n",
      "[NeMo I 2025-02-28 04:17:59 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "    Params for bucket 1 (1108204800 elements):\n",
      "    \tmodule.decoder.layers.24.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.22.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.19.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.16.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.14.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.12.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.9.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.7.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.20.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.18.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.15.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.13.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.5.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.5.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.18.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.16.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.13.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.11.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.9.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.6.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.22.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.21.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.20.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.14.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.9.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.8.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.18.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.23.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.15.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.13.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.11.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.8.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.4.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.24.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.19.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.5.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.4.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.17.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.15.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.13.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.10.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.8.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.6.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.2.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.1.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.22.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.18.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.13.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.12.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.7.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.23.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.20.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.19.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.12.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.11.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.7.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.4.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.1.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.24.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.15.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.14.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.9.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.6.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n",
      "    \tmodule.embedding.word_embeddings.weight\n",
      "    \tmodule.decoder.layers.20.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.19.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.16.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.12.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.9.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.7.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.5.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.4.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.23.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.21.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.17.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.17.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.11.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.8.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.6.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.22.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.11.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.9.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.6.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.23.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.19.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.15.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.13.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.7.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.5.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.1.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.24.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.23.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.21.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.16.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.13.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.11.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.8.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.6.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.17.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.12.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.10.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.0.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.23.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.21.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.13.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.10.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.8.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.6.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_proj.bias\n",
      "    \tmodule.decoder.layers.2.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.19.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.16.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.14.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.11.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.6.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.4.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.0.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.23.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.20.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.15.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.12.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.5.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.4.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.23.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.22.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.16.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.8.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.7.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.0.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.final_norm.weight\n",
      "    \tmodule.decoder.layers.22.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.16.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.14.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.12.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.9.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.5.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.4.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.18.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.10.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.24.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.22.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.19.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.16.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.14.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.21.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.20.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.12.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.8.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.6.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.4.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.21.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.18.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.16.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.13.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.9.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.6.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.23.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.20.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.16.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.15.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.10.self_attention.linear_proj.weight\n",
      "    \tmodule.decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.23.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.21.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.18.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.15.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.6.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.1.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.22.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.12.mixer.mixer.filter.h\n",
      "    \tmodule.decoder.layers.9.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.7.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.23.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.20.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.20.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.20.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.17.self_attention.linear_qkv.weight\n",
      "    \tmodule.decoder.layers.15.mixer.dense_projection.weight\n",
      "    \tmodule.decoder.layers.13.mixer.mixer.filter.gamma\n",
      "    \tmodule.decoder.layers.8.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.5.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.2.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.21.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.19.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.14.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.9.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "    \tmodule.decoder.layers.2.mixer.hyena_proj_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.0.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.18.mixer.dense.bias\n",
      "    \tmodule.decoder.layers.22.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.20.mixer.mixer.filter.p\n",
      "    \tmodule.decoder.layers.19.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.14.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.9.mlp.linear_fc2.weight\n",
      "    \tmodule.decoder.layers.7.mixer.dense.weight\n",
      "    \tmodule.decoder.layers.5.mixer.mixer.conv_bias\n",
      "    \tmodule.decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n",
      "    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.16.mixer.dense_projection.layer_norm_weight\n",
      "    \tmodule.decoder.layers.13.mixer.mixer.filter.R\n",
      "    \tmodule.decoder.layers.11.mixer.mixer.short_conv.short_conv_weight\n",
      "    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n",
      "    \tmodule.decoder.layers.2.mixer.dense_projection.layer_norm_weight\n",
      "[NeMo I 2025-02-28 04:17:59 nemo_logging:393] Doing selective restore from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False)\n",
      "[WARNING  | py.warnings        ]: /workspace/bionemo2/3rdparty/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "  checkpoint.load_state_dict(\n",
      "\n",
      "[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "  device = getattr(value, \"device\", None)\n",
      "\n",
      "[NeMo I 2025-02-28 04:18:00 nemo_logging:393] Restoring model weights from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False)\n",
      "[NeMo I 2025-02-28 04:18:00 nemo_logging:393] Finished restoring from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False), cleaning up.\n"
     ]
    }
   ],
   "source": [
    "!{predict_selected_seq_logits_command}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
